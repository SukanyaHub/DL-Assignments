{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IATzIJA0whys"
      },
      "outputs": [],
      "source": [
        "#importing necessary libraries\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cFeIeeS3NYH",
        "outputId": "62b7e354-5b10-47ae-eb8d-c5d70df0e78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "training images =  (60000, 28, 28) training labels =  (60000,)\n",
            "testing images =  (10000, 28, 28) testing labels =  (10000,)\n"
          ]
        }
      ],
      "source": [
        "#loading mnist dataset\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "print(\"training images = \", train_images.shape, \"training labels = \", train_labels.shape)\n",
        "print(\"testing images = \", test_images.shape, \"testing labels = \", test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "hxKPDcqa7wiM",
        "outputId": "0ee0bc03-0427-4feb-c2ab-5e316b82bab1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  38,  48,  48,  22,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         62,  97, 198, 243, 254, 254, 212,  27,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  67,\n",
              "        172, 254, 254, 225, 218, 218, 237, 248,  40,   0,  21, 164, 187,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  89, 219,\n",
              "        254,  97,  67,  14,   0,   0,  92, 231, 122,  23, 203, 236,  59,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  25, 217, 242,\n",
              "         92,   4,   0,   0,   0,   0,   4, 147, 253, 240, 232,  92,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 101, 255,  92,\n",
              "          0,   0,   0,   0,   0,   0, 105, 254, 254, 177,  11,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 167, 244,  41,\n",
              "          0,   0,   0,   7,  76, 199, 238, 239,  94,  10,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 192, 121,   0,\n",
              "          0,   2,  63, 180, 254, 233, 126,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 190, 196,  14,\n",
              "          2,  97, 254, 252, 146,  52,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 130, 225,  71,\n",
              "        180, 232, 181,  60,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 130, 254, 254,\n",
              "        230,  46,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   6,  77, 244, 254, 162,\n",
              "          4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0, 110, 254, 218, 254, 116,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0, 131, 254, 154,  28, 213,  86,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  66, 209, 153,  19,  19, 233,  60,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0, 142, 254, 165,   0,  14, 216, 167,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  90, 254, 175,   0,  18, 229,  92,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  26, 229, 249, 176, 222, 244,  44,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  73, 193, 197, 134,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-aeaf2ed1-3874-4906-8185-48a5c0f7a40e\" class=\"ndarray_repr\"><pre>ndarray (28, 28) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA60lEQVR4nGNgGEqAEV1AjYvh2SusSu0Sj33+9++KNDY55zX//j28devtDw0G0SW7UeUib/9LdOZjYIh5XiV++o01ipzkzU8xLAwMDAwsk/9+eBGDqjH1P1Qg89+/jdxoNi7/osnAwMDuc/zd+zgudOccqGRgYLLf8u9lHRa37jvCx5T4788kE2weaXrovuXFVhtsUgwMTf/+PdPDLsXm++XfIhZUMSYYI2PjM4ZLf7BrbP43S+ZqGHY5p4szhYVfojsGaqyPLs/b3++zsUueYeBiYOHdit1Yqac/N9z7ooNdksHz4NE2HFJUBwCmQlGEUS4nmQAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  38,  48,  48,  22,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         62,  97, 198, 243, 254, 254, 212,  27,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  67,\n",
              "        172, 254, 254, 225, 218, 218, 237, 248,  40,   0,  21, 164, 187,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  89, 219,\n",
              "        254,  97,  67,  14,   0,   0,  92, 231, 122,  23, 203, 236,  59,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  25, 217, 242,\n",
              "         92,   4,   0,   0,   0,   0,   4, 147, 253, 240, 232,  92,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 101, 255,  92,\n",
              "          0,   0,   0,   0,   0,   0, 105, 254, 254, 177,  11,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 167, 244,  41,\n",
              "          0,   0,   0,   7,  76, 199, 238, 239,  94,  10,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 192, 121,   0,\n",
              "          0,   2,  63, 180, 254, 233, 126,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 190, 196,  14,\n",
              "          2,  97, 254, 252, 146,  52,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 130, 225,  71,\n",
              "        180, 232, 181,  60,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 130, 254, 254,\n",
              "        230,  46,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   6,  77, 244, 254, 162,\n",
              "          4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0, 110, 254, 218, 254, 116,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0, 131, 254, 154,  28, 213,  86,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  66, 209, 153,  19,  19, 233,  60,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0, 142, 254, 165,   0,  14, 216, 167,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  90, 254, 175,   0,  18, 229,  92,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  26, 229, 249, 176, 222, 244,  44,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  73, 193, 197, 134,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-aeaf2ed1-3874-4906-8185-48a5c0f7a40e button').onclick = (e) => {\n",
              "        document.querySelector('#id-aeaf2ed1-3874-4906-8185-48a5c0f7a40e').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-aeaf2ed1-3874-4906-8185-48a5c0f7a40e button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "train_images[59999]  #displaying sample input data from training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfhEoJuY3X-g",
        "outputId": "07c39bac-9b36-46d5-d6db-606ea5b03f48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_labels[59999]  #displaying sample output data(label) from training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "KUBhUyfI670b",
        "outputId": "0c91999f-e0aa-444c-bc41-ec5d866abfd4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 16 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAAKyCAYAAADW/k9LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABerElEQVR4nO3dfXzPdf///8cbsw1jzkUZQiynmdODLKNJ0pScRFJRh1I7HIYcB+boDKGEnJQScSRfGZJDKlORnCRqMWYZkZM52UYYs9fvj3726fV+vtjL+/ne3ie7XS8Xl8vxvPd8vd4POzzx8Nrz+XIYhmEIAAAAAMBlxTxdAAAAAAD4OhorAAAAANBEYwUAAAAAmmisAAAAAEATjRUAAAAAaKKxAgAAAABNNFYAAAAAoInGCgAAAAA00VgBAAAAgKYi31ilpaWJw+GQqVOnuu2eGzduFIfDIRs3bnTbPYGiijUKeD/WKeDdWKOFwycbqw8++EAcDofs2LHD06UUiAkTJojD4VB+BAUFebo0wBZ/X6MiIkePHpXevXtLaGiolC1bVh588EH59ddfPV0WYFtRWKd/1aVLF3E4HDJs2DBPlwLY4u9rdN++fTJ8+HBp166dBAUFicPhkLS0NE+XpaWEpwvA9c2ZM0fKlCmTNy5evLgHqwFwzfnz5+Wee+6RzMxM+de//iUBAQHy5ptvSseOHWXXrl1SsWJFT5cI4C9WrFghW7Zs8XQZAP5iy5YtMmPGDAkPD5eGDRvKrl27PF2SNhorL9arVy+pVKmSp8sA4GT27NmSkpIi27Ztk5YtW4qIyH333SeNGjWSadOmyWuvvebhCgFcc+nSJRkxYoSMHj1axo8f7+lyAPz/evToIRkZGRISEiJTp071i8bKJ78V0I7Lly/L+PHjpUWLFlKuXDkpXbq0dOjQQRITE697zZtvvilhYWESHBwsHTt2lKSkJGVOcnKy9OrVSypUqCBBQUESEREhq1evzreeCxcuSHJyspw6dcr2z8EwDMnKyhLDMGxfA/gKX16jy5cvl5YtW+Y1VSIiDRo0kKioKFm2bFm+1wO+wpfX6TWvv/665ObmSlxcnO1rAF/hy2u0QoUKEhISku88X+K3jVVWVpbMnz9fIiMjZfLkyTJhwgRJT0+X6Ohoy4540aJFMmPGDHnuuedkzJgxkpSUJJ06dZITJ07kzfnll1+kTZs2snfvXnnxxRdl2rRpUrp0aYmJiZGEhIQb1rNt2zZp2LChzJo1y/bPoU6dOlKuXDkJCQmRAQMGmGoBfJ2vrtHc3Fz56aefJCIiQvlvrVq1ktTUVDl37py9LwLg5Xx1nV5z+PBhmTRpkkyePFmCg4Nv6ucO+AJfX6N+x/BBCxYsMETE2L59+3Xn5OTkGNnZ2abs7NmzRtWqVY0nn3wyLzt48KAhIkZwcLBx5MiRvHzr1q2GiBjDhw/Py6KioozGjRsbly5dystyc3ONdu3aGfXq1cvLEhMTDRExEhMTlSw+Pj7fn9/06dONYcOGGUuWLDGWL19uxMbGGiVKlDDq1atnZGZm5ns94Gn+vEbT09MNETFeeukl5b+9/fbbhogYycnJN7wH4A38eZ1e06tXL6Ndu3Z5YxExnnvuOVvXAp5WFNboNVOmTDFExDh48OBNXedt/PaJVfHixaVkyZIi8ue/MJ85c0ZycnIkIiJCdu7cqcyPiYmRGjVq5I1btWolrVu3lrVr14qIyJkzZ2TDhg3Su3dvOXfunJw6dUpOnTolp0+flujoaElJSZGjR49et57IyEgxDEMmTJiQb+2xsbEyc+ZMefTRR+Xhhx+W6dOny8KFCyUlJUVmz559k18JwDv56hq9ePGiiIgEBgYq/+3ayZ3X5gC+zlfXqYhIYmKifPLJJzJ9+vSb+0kDPsSX16g/8tvGSkRk4cKF0qRJEwkKCpKKFStK5cqV5bPPPpPMzExlbr169ZSsfv36ecc+HjhwQAzDkHHjxknlypVNP+Lj40VE5OTJkwX2c3n00UelWrVq8uWXXxbYZwCFzRfX6LVvJ8rOzlb+26VLl0xzAH/gi+s0JydHXnjhBXnsscdMeyEBf+SLa9Rf+e2pgIsXL5ZBgwZJTEyMjBw5UqpUqSLFixeXiRMnSmpq6k3fLzc3V0RE4uLiJDo62nJO3bp1tWrOz2233SZnzpwp0M8ACouvrtEKFSpIYGCgHDt2TPlv17Lq1atrfw7gDXx1nS5atEj27dsn8+bNU96Lc+7cOUlLS5MqVapIqVKltD8L8CRfXaP+ym8bq+XLl0udOnVkxYoV4nA48vJr3bazlJQUJdu/f7/UqlVLRP48SEJEJCAgQDp37uz+gvNhGIakpaVJ8+bNC/2zgYLgq2u0WLFi0rhxY8sXNm7dulXq1Knjd6ccoejy1XV6+PBhuXLlivztb39T/tuiRYtk0aJFkpCQIDExMQVWA1AYfHWN+iu//VbAay/TNf5yVPnWrVuv+4LAlStXmr5ndNu2bbJ161a57777RESkSpUqEhkZKfPmzbP8l+r09PQb1nMzx09a3WvOnDmSnp4uXbt2zfd6wBf48hrt1auXbN++3dRc7du3TzZs2CCPPPJIvtcDvsJX12nfvn0lISFB+SEi0q1bN0lISJDWrVvf8B6AL/DVNeqvfPqJ1fvvvy/r1q1T8tjYWOnevbusWLFCevbsKffff78cPHhQ5s6dK+Hh4XL+/Hnlmrp160r79u1l6NChkp2dLdOnT5eKFSvKqFGj8ua8/fbb0r59e2ncuLEMGTJE6tSpIydOnJAtW7bIkSNHZPfu3detddu2bXLPPfdIfHx8vhv6wsLCpE+fPtK4cWMJCgqSTZs2ydKlS6VZs2byzDPP2P8CAR7mr2v02WeflXfffVfuv/9+iYuLk4CAAHnjjTekatWqMmLECPtfIMAL+OM6bdCggTRo0MDyv9WuXZsnVfAp/rhGRUQyMzNl5syZIiKyefNmERGZNWuWhIaGSmhoqAwbNszOl8e7eOQsQk3Xjp+83o/ffvvNyM3NNV577TUjLCzMCAwMNJo3b26sWbPGePzxx42wsLC8e107fnLKlCnGtGnTjNtuu80IDAw0OnToYOzevVv57NTUVGPgwIFGtWrVjICAAKNGjRpG9+7djeXLl+fN0T1+cvDgwUZ4eLgREhJiBAQEGHXr1jVGjx5tZGVl6XzZgELj72vUMAzjt99+M3r16mWULVvWKFOmjNG9e3cjJSXF1S8ZUOiKwjp1Jhy3Dh/i72v0Wk1WP/5auy9xGMZfnh0CAAAAAG6a3+6xAgAAAIDCQmMFAAAAAJporAAAAABAE40VAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANJWwO9HhcBRkHfBRvAbNe7BGYYU16l1Yp7DCOvUerFFYsbtGeWIFAAAAAJporAAAAABAE40VAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAIAmGisAAAAA0ERjBQAAAACaaKwAAAAAQBONFQAAAABoorECAAAAAE00VgAAAACgicYKAAAAADTRWAEAAACAJhorAAAAANBEYwUAAAAAmmisAAAAAEBTCU8XAACFoUWLFko2bNgw03jgwIHKnEWLFinZzJkzlWznzp0a1QEAAF/HEysAAAAA0ERjBQAAAACaaKwAAAAAQBONFQAAAABochiGYdia6HAUdC0eV7x4cSUrV66cy/dz3hhfqlQpZc4dd9yhZM8995ySTZ061TTu16+fMufSpUtKNmnSJCX7z3/+oxbrIpu/fFAIisIatatZs2ZKtmHDBiUrW7asS/fPzMxUsooVK7p0r4LGGvUurFPPioqKMo2XLFmizOnYsaOS7du3r8BqEmGdehPWaMEYO3askln9fbRYMfMzn8jISGXO119/7ba67LK7RnliBQAAAACaaKwAAAAAQBONFQAAAABoorECAAAAAE0lPF2Arpo1aypZyZIllaxdu3ZK1r59e9M4NDRUmfPwww+7XpwNR44cUbIZM2YoWc+ePU3jc+fOKXN2796tZJ7Y4AcUplatWinZJ598omRWB9E4b0a1WleXL19WMquDKtq0aWMa79y509a9AFfdfffdSmb1azMhIaEwyvEJLVu2NI23b9/uoUoA/zVo0CAlGz16tJLl5ubmey9fO9iFJ1YAAAAAoInGCgAAAAA00VgBAAAAgCaf2mNl96WfOi/1LUhW30tq9cK08+fPK5nzSwyPHTumzDl79qySFfRLDYGC5PxS7bvuukuZs3jxYiW75ZZbXPq8lJQUJXv99deVbOnSpUq2efNm09hqbU+cONGlugArVi/OrFevnpIV1T1Wzi8aFRGpXbu2aRwWFqbM4QWxgB6rdRUUFOSBSgofT6wAAAAAQBONFQAAAABoorECAAAAAE00VgAAAACgyacOrzh8+LCSnT59WskK+vCKrVu3KllGRoaS3XPPPaax1ctBP/zwQ7fVBfibefPmmcb9+vUr0M+zOhyjTJkySmb14m3ngwSaNGnitroAKwMHDlSyLVu2eKAS72R1iM2QIUNMY6vDb5KTkwusJsAfde7c2TR+/vnnbV1ntda6d+9uGp84ccL1wjyAJ1YAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQ5FOHV5w5c0bJRo4cqWTOG99ERH788UclmzFjRr6fuWvXLiXr0qWLkv3xxx9Kduedd5rGsbGx+X4eUFS1aNFCye6//37T2OFw2LqX1eESn376qZJNnTrVNP7999+VOVa/d5w9e1bJOnXqZBrbrRVwVbFi/NvojcyfPz/fOSkpKYVQCeA/2rdvr2QLFiwwje0eIjdlyhQlO3TokGuFeQl+VwYAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAIAmnzq8wsrKlSuVbMOGDUp27tw5JWvatKlp/NRTTylznDe3i1gfVGHll19+MY2ffvppW9cB/q5Zs2ZK9sUXXyhZ2bJlTWPDMJQ5//vf/5SsX79+StaxY0clGzt2rGlstdk9PT1dyXbv3q1kubm5prHzwRsiInfddZeS7dy5U8kAZ02aNFGyqlWreqAS32FnA73V7zsAru/xxx9XsurVq+d73caNG5Vs0aJF7ijJq/DECgAAAAA00VgBAAAAgCYaKwAAAADQRGMFAAAAAJp8/vAKK1lZWbbmZWZm5jtnyJAhSvbxxx8rmfPGdQB/ql+/vpKNHDlSyaw2mp86dco0PnbsmDJn4cKFSnb+/Hkl++yzz2xl7hIcHKxkI0aMULL+/fsXWA3wH926dVMyq19jRZXVQR61a9fO97qjR48WRDmAX6hUqZKSPfnkk0rm/HfgjIwMZc4rr7zitrq8GU+sAAAAAEATjRUAAAAAaKKxAgAAAABNfrnHyq4JEyaYxi1atFDmWL1UtHPnzkq2fv16t9UF+KrAwEAls3rJttV+EauXeA8cONA03rFjhzLHl/aZ1KxZ09MlwEfdcccdtuY5v5i+qLD6fcZq39X+/ftNY6vfd4CiqFatWkr2ySefuHSvmTNnKlliYqJL9/I1PLECAAAAAE00VgAAAACgicYKAAAAADTRWAEAAACApiJ9eMUff/xhGlu9DHjnzp1K9u677yqZ1aY85432b7/9tjLHMIx86wR8RfPmzZXM6qAKKw8++KCSff3119o1AUXJ9u3bPV2ClrJlyypZ165dTeMBAwYoc+69915b93/55ZdNY6sXmQJFkfM6ExFp0qSJrWu/+uor0/itt95yS02+iCdWAAAAAKCJxgoAAAAANNFYAQAAAIAmGisAAAAA0FSkD69wlpqaqmSDBg1SsgULFijZY489lm9WunRpZc6iRYuU7NixYzcqE/Bab7zxhpI5HA4lszqUwtcPqihWzPzvVLm5uR6qBEVZhQoV3Havpk2bKpnVeu7cubNpfOuttypzSpYsqWT9+/dXMud1JCJy8eJF03jr1q3KnOzsbCUrUUL9K84PP/ygZEBRExMTo2STJk2yde2mTZuU7PHHHzeNMzMzXarLH/DECgAAAAA00VgBAAAAgCYaKwAAAADQRGMFAAAAAJo4vCIfCQkJSpaSkqJkVpv2o6KiTOPXXntNmRMWFqZkr776qpIdPXr0hnUCntC9e3fTuFmzZsocwzCUbPXq1QVVksc4H1Zh9fPetWtXIVUDf+N8gIOI9a+xuXPnKtm//vUvlz6zSZMmSmZ1eEVOTo5pfOHCBWXOnj17lOz9999Xsh07diiZ88E2J06cUOYcOXJEyYKDg5UsOTlZyQB/V6tWLdP4k08+cflev/76q5JZrcmiiidWAAAAAKCJxgoAAAAANNFYAQAAAIAmGisAAAAA0MThFS5ISkpSst69eyvZAw88YBovWLBAmfPMM88oWb169ZSsS5cuN1MiUCicN4eXLFlSmXPy5Ekl+/jjjwusJncLDAxUsgkTJuR73YYNG5RszJgx7igJRdCzzz6rZIcOHVKydu3aue0zDx8+rGQrV65Usr1795rG33//vdtqsPL0008rWeXKlZXMapM9UBSNHj3aNHY+bOlmTJo0Sbccv8YTKwAAAADQRGMFAAAAAJporAAAAABAE3us3CQjI0PJPvzwQ9N4/vz5ypwSJdT/C+6++24li4yMNI03btx4U/UBnpKdna1kx44d80Al+bPaTzV27FglGzlypJI5v6B02rRpypzz589rVAeYTZ482dMleERUVJSteTovQQV8VbNmzZTs3nvvdeleq1atUrJ9+/a5dK+igidWAAAAAKCJxgoAAAAANNFYAQAAAIAmGisAAAAA0MThFS5o0qSJkvXq1UvJWrZsaRpbHVRhZc+ePUr2zTff2KwO8C6rV6/2dAnX5bzJ1+pQij59+iiZ1Ybehx9+2G11AdCXkJDg6RKAQrd+/XolK1++fL7XWb3Ye9CgQe4oqUjhiRUAAAAAaKKxAgAAAABNNFYAAAAAoInGCgAAAAA0cXjFX9xxxx1KNmzYMCV76KGHlKxatWoufebVq1eV7NixY0qWm5vr0v2BguRwOG44FhGJiYlRstjY2IIq6bqGDx+uZOPGjTONy5Urp8xZsmSJkg0cONB9hQEA4CYVK1ZUMjt/h5w9e7aSnT9/3i01FSU8sQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAICmInN4hdXhEv369TONrQ6qqFWrlttq2LFjh5K9+uqrSrZ69Wq3fSZQkAzDuOFYxHrtzZgxQ8nef/99JTt9+rRp3KZNG2XOY489pmRNmzZVsltvvVXJDh8+bBp//vnnyhyrDb0AvIvVwTn169dXsu+//74wygEKxYIFC5SsWDHXnpl89913uuVAeGIFAAAAANporAAAAABAE40VAAAAAGjy+T1WVatWVbLw8HAlmzVrlpI1aNDAbXVs3bpVyaZMmWIar1q1SpnDi3/h74oXL65kzz77rJI9/PDDSpaVlWUa16tXz+U6rL5/PDEx0TQeP368y/cH4DlW+ztd3WsCeKNmzZopWefOnZXM6u+Vly9fNo3ffvttZc6JEydcLw55+F0HAAAAADTRWAEAAACAJhorAAAAANBEYwUAAAAAmrz68IoKFSqYxvPmzVPmWG3mq1OnjttqsNrwPm3aNCWzerHoxYsX3VYH4I22bNliGm/fvl2Z07JlS1v3snqRsNXhNM6cXyIsIrJ06VIli42NtVUHAP/Qtm1bJfvggw8KvxDADUJDQ5XM6s9NK0ePHjWN4+Li3FESLPDECgAAAAA00VgBAAAAgCYaKwAAAADQRGMFAAAAAJo8cnhF69atlWzkyJFK1qpVK9O4Ro0abq3jwoULpvGMGTOUOa+99pqS/fHHH26tA/BVR44cMY0feughZc4zzzyjZGPHjnXp89566y0lmzNnjpIdOHDApfsD8E0Oh8PTJQAAT6wAAAAAQBeNFQAAAABoorECAAAAAE00VgAAAACgySOHV/Ts2dNWZseePXuUbM2aNUqWk5OjZNOmTTONMzIyXKoBwJ+OHTumZBMmTLCVAYAd//vf/5TskUce8UAlQOFJTk5Wsu+++07J2rdvXxjl4Dp4YgUAAAAAmmisAAAAAEATjRUAAAAAaKKxAgAAAABNDsMwDFsTeas5LNj85YNCwBqFFdaod2Gdwgrr1HuwRmHF7hrliRUAAAAAaKKxAgAAAABNNFYAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQRGMFAAAAAJporAAAAABAE40VAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAIAmh2EYhqeLAAAAAABfxhMrAAAAANBEYwUAAAAAmmisAAAAAEATjRUAAAAAaKKxAgAAAABNNFYAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQRGMFAAAAAJporAAAAABAE40VAAAAAGiisQIAAAAATTRWAAAAAKCpyDdWaWlp4nA4ZOrUqW6758aNG8XhcMjGjRvddk+gqGKNAt6PdQp4N9Zo4fDJxuqDDz4Qh8MhO3bs8HQpBWbp0qVy1113SVBQkFSuXFmeeuopOXXqlKfLAmzx9zW6YsUK6dOnj9SpU0dKlSold9xxh4wYMUIyMjI8XRpgm7+v03379snw4cOlXbt2EhQUJA6HQ9LS0jxdFmCbv6/RhIQEiY6OlurVq0tgYKDceuut0qtXL0lKSvJ0aS7zycbK382ZM0f69esnFSpUkDfeeEOGDBkiS5culaioKLl06ZKnywOKvKefflr27t0rAwYMkBkzZkjXrl1l1qxZ0rZtW7l48aKnywMgIlu2bJEZM2bIuXPnpGHDhp4uB4CTn3/+WcqXLy+xsbEye/ZsGTp0qPz444/SqlUr2b17t6fLc0kJTxcAs8uXL8u//vUvufvuu+WLL74Qh8MhIiLt2rWTBx54QN599115/vnnPVwlULQtX75cIiMjTVmLFi3k8ccflyVLlsjgwYM9UxiAPD169JCMjAwJCQmRqVOnyq5duzxdEoC/GD9+vJINHjxYbr31VpkzZ47MnTvXA1Xp8dsnVpcvX5bx48dLixYtpFy5clK6dGnp0KGDJCYmXveaN998U8LCwiQ4OFg6duxo+SgyOTlZevXqJRUqVJCgoCCJiIiQ1atX51vPhQsXJDk5Od9v50tKSpKMjAzp06dPXlMlItK9e3cpU6aMLF26NN/PAnyBr65REVGaKhGRnj17iojI3r17870e8BW+vE4rVKggISEh+c4DfJkvr1ErVapUkVKlSvnst9b7bWOVlZUl8+fPl8jISJk8ebJMmDBB0tPTJTo62vJfrRYtWiQzZsyQ5557TsaMGSNJSUnSqVMnOXHiRN6cX375Rdq0aSN79+6VF198UaZNmyalS5eWmJgYSUhIuGE927Ztk4YNG8qsWbNuOC87O1tERIKDg5X/FhwcLD/++KPk5uba+AoA3s1X1+j1HD9+XEREKlWq5NL1gDfyt3UK+Bt/WKMZGRmSnp4uP//8swwePFiysrIkKirK9vVexfBBCxYsMETE2L59+3Xn5OTkGNnZ2abs7NmzRtWqVY0nn3wyLzt48KAhIkZwcLBx5MiRvHzr1q2GiBjDhw/Py6KioozGjRsbly5dystyc3ONdu3aGfXq1cvLEhMTDRExEhMTlSw+Pv6GP7f09HTD4XAYTz31lClPTk42RMQQEePUqVM3vAfgaf68Rq/nqaeeMooXL27s37/fpeuBwlaU1umUKVMMETEOHjx4U9cBnlRU1ugdd9yR93fcMmXKGGPHjjWuXr1q+3pv4rdPrIoXLy4lS5YUEZHc3Fw5c+aM5OTkSEREhOzcuVOZHxMTIzVq1Mgbt2rVSlq3bi1r164VEZEzZ87Ihg0bpHfv3nLu3Dk5deqUnDp1Sk6fPi3R0dGSkpIiR48evW49kZGRYhiGTJgw4YZ1V6pUSXr37i0LFy6UadOmya+//irffvut9OnTRwICAkRE2BwPv+Cra9TKf//7X3nvvfdkxIgRUq9evZu+HvBW/rROAX/kD2t0wYIFsm7dOpk9e7Y0bNhQLl68KFevXrV9vTfx68MrrjUnycnJcuXKlby8du3aylyrvwzVr19fli1bJiIiBw4cEMMwZNy4cTJu3DjLzzt58qTpF6ur5s2bJxcvXpS4uDiJi4sTEZEBAwbI7bffLitWrJAyZcpofwbgDXx1jf7Vt99+K0899ZRER0fLq6++6tZ7A97AH9Yp4M98fY22bds273/37ds37xRPd75zq7D4bWO1ePFiGTRokMTExMjIkSOlSpUqUrx4cZk4caKkpqbe9P2u7WuKi4uT6Ohoyzl169bVqvmacuXKyapVq+Tw4cOSlpYmYWFhEhYWJu3atZPKlStLaGioWz4H8CRfXqPX7N69W3r06CGNGjWS5cuXS4kSfvtbKooof1ingD/ztzVavnx56dSpkyxZsoTGypssX75c6tSpIytWrDCdrhcfH285PyUlRcn2798vtWrVEhGROnXqiIhIQECAdO7c2f0FW6hZs6bUrFlTRP7c2PfDDz/Iww8/XCifDRQ0X1+jqamp0rVrV6lSpYqsXbuWJ8nwS76+TgF/549r9OLFi5KZmemRz9bl13usREQMw8jLtm7dKlu2bLGcv3LlStP3jG7btk22bt0q9913n4j8efxjZGSkzJs3T44dO6Zcn56efsN6dI+fHDNmjOTk5Mjw4cNduh7wNr68Ro8fPy733nuvFCtWTD7//HOpXLlyvtcAvsiX1ylQFPjyGj158qSSpaWlyVdffSURERH5Xu+NfPqJ1fvvvy/r1q1T8tjYWOnevbusWLFCevbsKffff78cPHhQ5s6dK+Hh4XL+/Hnlmrp160r79u1l6NChkp2dLdOnT5eKFSvKqFGj8ua8/fbb0r59e2ncuLEMGTJE6tSpIydOnJAtW7bIkSNHbviW6G3btsk999wj8fHx+W7omzRpkiQlJUnr1q2lRIkSsnLlSlm/fr288sor0rJlS/tfIMDD/HWNdu3aVX799VcZNWqUbNq0STZt2pT336pWrSpdunSx8dUBvIO/rtPMzEyZOXOmiIhs3rxZRERmzZoloaGhEhoaKsOGDbPz5QE8zl/XaOPGjSUqKkqaNWsm5cuXl5SUFHnvvffkypUrMmnSJPtfIG/imcMI9Vw7fvJ6P3777TcjNzfXeO2114ywsDAjMDDQaN68ubFmzRrj8ccfN8LCwvLude34ySlTphjTpk0zbrvtNiMwMNDo0KGDsXv3buWzU1NTjYEDBxrVqlUzAgICjBo1ahjdu3c3li9fnjdH9/jJNWvWGK1atTJCQkKMUqVKGW3atDGWLVum8yUDCpW/r9Eb/dw6duyo8ZUDCo+/r9NrNVn9+GvtgLfy9zUaHx9vREREGOXLlzdKlChhVK9e3ejbt6/x008/6XzZPMphGH95dggAAAAAuGl+u8cKAAAAAAoLjRUAAAAAaKKxAgAAAABNNFYAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQRGMFAAAAAJpK2J3ocDgKsg74KN4v7T1Yo7DCGvUurFNYYZ16D9YorNhdozyxAgAAAABNNFYAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQRGMFAAAAAJporAAAAABAE40VAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAIAmGisAAAAA0ERjBQAAAACaaKwAAAAAQBONFQAAAABoorECAAAAAE0lPF0AAADwH2+99ZaSvfDCC0qWlJSkZN27d1eyQ4cOuacwAChgPLECAAAAAE00VgAAAACgicYKAAAAADTRWAEAAACAJg6vAFAkhISEKFmZMmVM4/vvv1+ZU7lyZSV74403lCw7O1ujOsB31apVyzQeMGCAMic3N1fJGjZsqGQNGjRQMg6vAPTUr1/fNA4ICFDm3H333Uo2e/ZsJbNay+60atUq07hv377KnMuXLxdoDTp4YgUAAAAAmmisAAAAAEATjRUAAAAAaKKxAgAAAABNHF4BwKc5b5wXERk9erSStW3bVskaNWrk0mfecsstSvbCCy+4dC/A16Wnp5vG33zzjTKnR48ehVUOUGTceeedSjZo0CAle+SRR0zjYsXU5yrVq1dXMquDKgzDuIkKb57z7xVz585V5vzjH/9QsqysrIIq6abwxAoAAAAANNFYAQAAAIAmGisAAAAA0MQeq79o3bq1klm96LBjx45KZvV9rs7i4uKU7Pfff1ey9u3bK9nixYtN461bt+b7eYCvc35ZqNX3Vffv31/JgoODlczhcCjZb7/9ZhqfO3dOmWP1EtPevXsrmfOLFJOTk5U5gD/6448/TGNe6AsUjokTJypZt27dPFBJwRk4cKCSvffee0q2efPmwignXzyxAgAAAABNNFYAAAAAoInGCgAAAAA00VgBAAAAgKYifXhFnz59TOO33npLmVOpUiUls9oEv3HjRiWrXLmyaTxlyhRbdVnd3/leffv2tXUvwBuVK1dOySZPnqxkzms0JCTE5c9MSUlRsujoaNM4ICBAmWN1CIXV7wtWGVAUhIaGmsZNmzb1TCFAEfPFF18omZ3DK06ePKlkVgdCWL1I2Oqlwc7atWunZFYHv/kjnlgBAAAAgCYaKwAAAADQRGMFAAAAAJporAAAAABAk18eXlGihPrTioiIULJ3333XNC5VqpQy55tvvlGyl19+Wck2bdqkZIGBgabxsmXLlDn33nuvklnZsWOHrXmAL+jZs6eSDR482G33T01NVbIuXboo2W+//WYa161b1201AEWF85+dNWvWdPleLVu2VDLnA2QOHTrk8v0BfzJnzhwlW7lyZb7XXblyRcmOHz/ujpJERKRs2bJKlpSUpGTVq1fP915WPx9v/jsxT6wAAAAAQBONFQAAAABoorECAAAAAE00VgAAAACgyS8PrxgwYICSzZ8/P9/rrN5g3adPHyXLysqyVYfztXYPqjhy5IiSLVy40Na1gC945JFHXLouLS1NybZv365ko0ePVjLngyqsNGzY0KW6gKLs999/N40/+OADZc6ECRNs3ctqXkZGhmk8a9Ysm5UB/i0nJ0fJ7PxZV9Cio6OVrHz58i7dy+rvxNnZ2S7dqzDwxAoAAAAANNFYAQAAAIAmGisAAAAA0ERjBQAAAACafP7wipdfflnJ/vWvfymZYRhKNnv2bNN47Nixyhy7B1VY+fe//+3SdS+88IKSpaenu1wH4G2GDBmiZE8//bSSrV+/3jQ+cOCAMufkyZNuq6tq1apuuxdQVFn9uWz38AoAvqdv376msdWf8cHBwS7de/z48S5d5yk8sQIAAAAATTRWAAAAAKCJxgoAAAAANPnUHiur77O02k91+fJlJfv888+VzPklohcvXrRVR1BQkJJZvfy3Zs2aprHD4VDmvPLKK0q2atUqW3UAvsr5haIi3rEHo23btp4uAfBLxYqp/46bm5vrgUoA2NW/f38le/HFF5Wsbt26pnFAQIDLn7lr1y7T+MqVKy7fyxN4YgUAAAAAmmisAAAAAEATjRUAAAAAaKKxAgAAAABNXn14RWhoqGn87LPPKnOsXvxrdVBFTEyMSzU4b8gTEVmyZImStWjRIt97LV++XMlef/11l+oC8CerF2qXLl3apXs1btzY1rzvvvtOybZs2eLSZwJFgdVBFVZ/fgOwr1atWkr22GOPKVnnzp1dun/79u2VzNV1m5WVpWRWB2GsXbvWNLZ7sJy34IkVAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANHn14RUlS5Y0jStVqmTrOqvN7FWqVFGyJ554wjTu0aOHMqdRo0ZKVqZMGSWz2sznnC1evFiZ88cffygZUBSVKlVKycLDw03j+Ph4ZU63bt1s3b9YMfXfkaw21Dv7/ffflcz59w4RkatXr9qqAwCAm2X199HVq1crWc2aNQujnJv27bffKtk777zjgUoKFk+sAAAAAEATjRUAAAAAaKKxAgAAAABNNFYAAAAAoMmrD6+4fPmyaZyenq7MqVy5spIdPHhQyVx9U7TVxnWrt0ffcsstSnbq1CnT+NNPP3WpBsCXBQQEKFnz5s2V7JNPPlEy53Vl9QZ2qzW6ZcsWJevatauSWR2Y4axECfW3yYceekjJ3nrrLdPY+fcvAADcyeFw2Mpc5eqhT1a6d++uZPfdd5+S/e9//3Pp/t6CJ1YAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQ5NWHV2RkZJjGMTExypw1a9YoWYUKFZQsNTVVyVatWmUaf/DBB8qcM2fOKNnSpUuVzOrwCqt5gD8rWbKkklkdGrFixQpb9/vPf/5jGm/YsEGZs3nzZiWz+j3A6lqrN9k7szogZ+LEiUp2+PBh03jlypXKnOzs7Hw/D/BHOpvg7777btN41qxZbqkJ8CVJSUlKFhkZqWQDBgxQss8//9w0vnTpktvqEhF56qmnTOPnn3/erff3JTyxAgAAAABNNFYAAAAAoInGCgAAAAA0OQybb8515wvHfInz93aLiHz99ddKZvW94v/4xz9M45kzZ7qtLm/h6ouX4X6eWKPOL/996aWXlDkjR460dS+rlwI+9thjprHzvksR6z1Qa9euVbK77rpLyZxf4vv6668rc6z2YT344INK5uzLL79UssmTJyvZ2bNn872XiMiuXbtszXPGGvUuRfXP0qtXryqZq782mzRpomR79uxx6V7egnXqPYrqGtVRrlw50/j06dO2rnvggQeUzFtfEGx3jfLECgAAAAA00VgBAAAAgCYaKwAAAADQRGMFAAAAAJq8+gXB3iA4OFjJrA6qsNrUxguC4U+KFy+uZC+//LJpHBcXp8z5448/lOzFF19UMqv14nxYRUREhDLH6mWhzZs3V7KUlBQlGzp0qGmcmJiozClbtqyStWvXTsn69+9vGvfo0UOZ88UXXyiZld9++03JateubetawBvNnTtXyZ555hmX7vX0008rmfNhUQAKT3R0tKdL8Bo8sQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAIAmDq/Ix+eff+7pEgCvYLVh3PmwigsXLihzrDaor1+/XsnatGmjZE888YRpfN999ylzrA6Yeemll5RswYIFSmZ1SISzrKwsJVu3bl2+Wb9+/ZQ5jz76aL6fJyIyfPhwW/MAX5GcnOzpEgCvFRAQYBrfe++9ypwNGzYo2cWLFwusputx/nNZROStt94q9Dq8FU+sAAAAAEATjRUAAAAAaKKxAgAAAABNNFYAAAAAoMlhGIZha6LDUdC1eCWrt0mvXbtWyay+jLfccotpnJ6e7r7CvITNXz4oBAW9Ro8dO6ZklStXNo2zs7OVOVab1kuXLq1kdevWdamuCRMmKNnEiROV7OrVqy7d39exRr1LUf2z1Mr+/fuV7Pbbb8/3umLF1H8Ttvr9IzU11bXCPIB16j0Keo22b99eyf7973+bxl26dFHm1K5dW8nsHMBkV4UKFZSsW7duSjZz5kwlCwkJyff+Vgdt9OjRQ8kSExPzvZcn2F2jPLECAAAAAE00VgAAAACgicYKAAAAADTxguB81KlTx9MlAF7h+PHjSua8xyowMFCZ07RpU1v3t9q7+M0335jGK1euVOakpaUpWVHdTwX4kl9++UXJ7PyZm5ubWxDlAIVi1qxZStaoUaN8rxs1apSSnTt3zi01iVjv67rrrruUzM5eo40bNyrZnDlzlMxb91Pp4IkVAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANHF4RT6+/fZbJbN6OSGbaeHv7r77biWLiYkxja02up48eVLJ3n//fSU7e/askl2+fPkmKgTgS9555x0le+CBBzxQCeD9hg4d6ukSRMT6z/RPP/3UNI6NjVXmXLp0qcBq8iY8sQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAIAmh2HnFcoi4nA4CroWn7F//34ls3pbfPv27U3j77//vsBq8hSbv3xQCFijsMIa9S6s0/8TFhamZGvWrFGyhg0bmsZWX8P69esrWWpqqkZ1hYt16j0Keo02a9ZMyZ5//nnT+PHHHy/QGqzWxoULF5TM6gA3q0NnkpKS3FOYF7O7RnliBQAAAACaaKwAAAAAQBONFQAAAABoorECAAAAAE0cXuGCQYMGKdn8+fOV7OuvvzaNnTcniojs2bPHbXV5AhtuvQdrFFZYo96FdQorrFPv4Yk1GhgYaBpb/T3zlVdeUbLy5csr2cqVK5Xsiy++MI1XrVqlzDl+/Hg+VRZtHF4BAAAAAIWExgoAAAAANNFYAQAAAIAmGisAAAAA0MThFS4oW7aski1btkzJOnfubBqvWLFCmfPEE08o2R9//KFRXeFiw633YI3CCmvUu7BOYYV16j1Yo7DC4RUAAAAAUEhorAAAAABAE40VAAAAAGhij5WbWO27evXVV03joUOHKnOaNGmiZL700mC+L9x7sEZhhTXqXVinsMI69R6sUVhhjxUAAAAAFBIaKwAAAADQRGMFAAAAAJporAAAAABAE4dXQAsbbr0HaxRWWKPehXUKK6xT78EahRUOrwAAAACAQkJjBQAAAACaaKwAAAAAQBONFQAAAABosn14BQAAAADAGk+sAAAAAEATjRUAAAAAaKKxAgAAAABNNFYAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQRGMFAAAAAJporAAAAABAE40VAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAIAmGisAAAAA0FTkG6u0tDRxOBwydepUt91z48aN4nA4ZOPGjW67J1BUsUYB78c6Bbwba7Rw+GRj9cEHH4jD4ZAdO3Z4upQCsW/fPhk+fLi0a9dOgoKCxOFwSFpamqfLAmzz9zXqrEuXLuJwOGTYsGGeLgWwzd/XaUJCgkRHR0v16tUlMDBQbr31VunVq5ckJSV5ujTAFn9foxMmTBCHw6H8CAoK8nRpLivh6QKg2rJli8yYMUPCw8OlYcOGsmvXLk+XBOA6VqxYIVu2bPF0GQCc/Pzzz1K+fHmJjY2VSpUqyfHjx+X999+XVq1ayZYtW6Rp06aeLhGAiMyZM0fKlCmTNy5evLgHq9FDY+WFevToIRkZGRISEiJTp06lsQK81KVLl2TEiBEyevRoGT9+vKfLAfAXVmty8ODBcuutt8qcOXNk7ty5HqgKgLNevXpJpUqVPF2GW/jktwLacfnyZRk/fry0aNFCypUrJ6VLl5YOHTpIYmLida958803JSwsTIKDg6Vjx46W3y6QnJwsvXr1kgoVKkhQUJBERETI6tWr863nwoULkpycLKdOncp3boUKFSQkJCTfeYAv8+U1es3rr78uubm5EhcXZ/sawJf4wzr9qypVqkipUqUkIyPDpesBb+MPa9QwDMnKyhLDMGxf4638trHKysqS+fPnS2RkpEyePFkmTJgg6enpEh0dbfkEaNGiRTJjxgx57rnnZMyYMZKUlCSdOnWSEydO5M355ZdfpE2bNrJ371558cUXZdq0aVK6dGmJiYmRhISEG9azbds2adiwocyaNcvdP1XAJ/n6Gj18+LBMmjRJJk+eLMHBwTf1cwd8ha+vUxGRjIwMSU9Pl59//lkGDx4sWVlZEhUVZft6wJv5wxqtU6eOlCtXTkJCQmTAgAGmWnyO4YMWLFhgiIixffv2687JyckxsrOzTdnZs2eNqlWrGk8++WRedvDgQUNEjODgYOPIkSN5+datWw0RMYYPH56XRUVFGY0bNzYuXbqUl+Xm5hrt2rUz6tWrl5clJiYaImIkJiYqWXx8/E39XKdMmWKIiHHw4MGbug7wpKKwRnv16mW0a9cubywixnPPPWfrWsAbFIV1ahiGcccddxgiYoiIUaZMGWPs2LHG1atXbV8PeIq/r9Hp06cbw4YNM5YsWWIsX77ciI2NNUqUKGHUq1fPyMzMzPd6b+S3T6yKFy8uJUuWFBGR3NxcOXPmjOTk5EhERITs3LlTmR8TEyM1atTIG7dq1Upat24ta9euFRGRM2fOyIYNG6R3795y7tw5OXXqlJw6dUpOnz4t0dHRkpKSIkePHr1uPZGRkWIYhkyYMMG9P1HAR/nyGk1MTJRPPvlEpk+ffnM/acDH+PI6vWbBggWybt06mT17tjRs2FAuXrwoV69etX094M18eY3GxsbKzJkz5dFHH5WHH35Ypk+fLgsXLpSUlBSZPXv2TX4lvIPfNlYiIgsXLpQmTZpIUFCQVKxYUSpXriyfffaZZGZmKnPr1aunZPXr18875vzAgQNiGIaMGzdOKleubPoRHx8vIiInT54s0J8P4G98cY3m5OTICy+8II899pi0bNlS+36At/PFdfpXbdu2lejoaBk6dKh8/vnnsnjxYhkzZoxbPwPwJF9fo3/16KOPSrVq1eTLL78ssM8oSH57KuDixYtl0KBBEhMTIyNHjpQqVapI8eLFZeLEiZKamnrT98vNzRURkbi4OImOjracU7duXa2agaLEV9fookWLZN++fTJv3jzl/XLnzp2TtLS0vA3ygK/z1XV6PeXLl5dOnTrJkiVL3PqiVMBT/G2NiojcdtttcubMmQL9jILit43V8uXLpU6dOrJixQpxOBx5+bVu21lKSoqS7d+/X2rVqiUif26sExEJCAiQzp07u79goIjx1TV6+PBhuXLlivztb39T/tuiRYtk0aJFkpCQIDExMQVWA1BYfHWd3sjFixct/yUf8EX+tkYNw5C0tDRp3rx5oX+2O/jttwJee7mY8ZejG7du3XrdF3muXLnS9D2j27Ztk61bt8p9990nIn8e0RoZGSnz5s2TY8eOKdenp6ffsB7dI2IBf+Ora7Rv376SkJCg/BAR6datmyQkJEjr1q1veA/AV/jqOhWx/naltLQ0+eqrryQiIiLf6wFf4Mtr1Opec+bMkfT0dOnatWu+13sjn35i9f7778u6deuUPDY2Vrp37y4rVqyQnj17yv333y8HDx6UuXPnSnh4uJw/f165pm7dutK+fXsZOnSoZGdny/Tp06VixYoyatSovDlvv/22tG/fXho3bixDhgyROnXqyIkTJ2TLli1y5MgR2b1793Vr3bZtm9xzzz0SHx+f74a+zMxMmTlzpoiIbN68WUREZs2aJaGhoRIaGirDhg2z8+UBPM4f12iDBg2kQYMGlv+tdu3aPKmCz/HHdSoi0rhxY4mKipJmzZpJ+fLlJSUlRd577z25cuWKTJo0yf4XCPAwf12jYWFh0qdPH2ncuLEEBQXJpk2bZOnSpdKsWTN55pln7H+BvIlHziLUdO34yev9+O2334zc3FzjtddeM8LCwozAwECjefPmxpo1a4zHH3/cCAsLy7vXteMnp0yZYkybNs247bbbjMDAQKNDhw7G7t27lc9OTU01Bg4caFSrVs0ICAgwatSoYXTv3t1Yvnx53hzd4yev1WT146+1A97K39eoFeG4dfgYf1+n8fHxRkREhFG+fHmjRIkSRvXq1Y2+ffsaP/30k86XDSg0/r5GBw8ebISHhxshISFGQECAUbduXWP06NFGVlaWzpfNoxyG4QevOQYAAAAAD/LbPVYAAAAAUFhorAAAAABAE40VAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAICmEnYnOhyOgqwDPorXoHkP1iissEa9C+sUVlin3oM1Cit21yhPrAAAAABAE40VAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAIAmGisAAAAA0ERjBQAAAACaaKwAAAAAQBONFQAAAABoorECAAAAAE00VgAAAACgicYKAAAAADTRWAEAAACAJhorAAAAANBEYwUAAAAAmmisAAAAAEATjRUAAAAAaCrh6QIAoKj46quvlMzhcChZp06dCqMcwC3Cw8NN4+7duytznn76aSXbvn27kv3444/5ft706dOV7PLly/leBwAFjSdWAAAAAKCJxgoAAAAANNFYAQAAAIAmGisAAAAA0MThFX8REBCgZO3atVOy1157Tcn+9re/FUhNAHzTm2++qWRWv58sWrSoMMoB3OKZZ55RsqlTp5rGZcqUsXWv22+/Xcn69u2b73VWh14kJiba+kwAKEg8sQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAIAmh2EYhq2JDkdB1+JxlSpVUrKTJ08q2fHjx5XsrrvusjXP39j85YNCUBTWqDebNGmSaRwbG6vMuXLlipINHjxYyZYtW+a2ulij3sXX12mFChWUbO/evaZxlSpVCrSGjIwMJevTp4+SrV+/vkDrcCfWqffw9TWKgmF3jfLECgAAAAA00VgBAAAAgCYaKwAAAADQxAuCXVCtWjVbWVHYYwXgT23atDGNrV44vmnTJiVz534qoKCdOXNGyeLj403jadOmKXNKlSqlZIcPH1aymjVr5ltDaGioknXt2lXJfGmPFYA/hYWFmcbBwcHKnH79+inZ0KFDbd3/s88+M42feOKJm6gufzyxAgAAAABNNFYAAAAAoInGCgAAAAA00VgBAAAAgCYOr3ABL48DPOPuu+9Wsn//+99KZrWx1WrTvaus7t+oUSPTODU1VZkTFxfnthoAbzF37lzT+O9//7syp2nTpkqWlZXlthpmzZrltnsBcL/OnTsr2UMPPaRkzn++litXTpmj80Jt54Om3I0nVgAAAACgicYKAAAAADTRWAEAAACAJhorAAAAANDE4RUusNo0FxQU5IFKgKLlnXfeUbJ69eopWXh4uJJt2rTJbXX861//UrKKFSuaxkOGDFHm7N692201AN7qlVdeUTKrQ2aaNWvmts8sWbKk2+4F4ObMnz/fNG7cuLEyp2XLli7d+9y5c0q2ZMkSJdu+fbuSffTRR0p26dIll+qwiydWAAAAAKCJxgoAAAAANNFYAQAAAIAmGisAAAAA0MThFW4SERGhZN9//70HKgH814ULF5SsoA+TsdpgHxYWpmS5ubkFVgPgS5YvX65kVofHrF+/XsmsNr3bYXVgRq9evVy6F4A/OR/KJCIyceJEJXvyySdN4zNnzihzfvjhByWbNGmSkiUlJZnGFy9eVOYcPnxYLdZL8MQKAAAAADTRWAEAAACAJhorAAAAANBEYwUAAAAAmji84i9ycnKULDMzU8nKlSunZLfffnuB1AQUZS+//LJpbLWxfe/evUq2e/dulz6vdOnSSjZ69GglK1WqlJI5H1ZjtYEfKAr69++vZE2bNlWyRo0aue0zrQ7HAKBn3LhxSvbUU08p2cyZM03jf//738qc8+fPu68wL8YTKwAAAADQRGMFAAAAAJporAAAAABAE3us/iIjI0PJvv32WyXr3r17IVQDFC233Xabkg0ZMsQ0ttoHOWzYMCVLT093qYY33nhDyR555BEl+/3335Xsb3/7m0ufCfiSBg0aKFlCQoJpXLduXWVOiRIF+9eN1atXF+j9AV/mvC/Yau/wY489pmT/+Mc/lCwxMVHJPv/8c9P40qVLN1mh/+CJFQAAAABoorECAAAAAE00VgAAAACgicYKAAAAADRxeAWAQmf1YlDnDfAiIpUqVTKNnV9CKCLy9ddfu1xHXFycaTxo0CBb17366qsufybgyxo2bKhktWvXNo0L+qAKK8OHD1ey559/vtDrALzR2LFjTWOrwyuWLVumZOvXr1eyonwwhR08sQIAAAAATTRWAAAAAKCJxgoAAAAANNFYAQAAAIAmDq9wk4oVK3q6BMDjrDatDxgwQMnee+89JStWTP13ntzcXNO4bdu2ypwxY8Yo2RtvvKFkFSpUULJHHnnENHY4HMqcRYsWKdm8efOUDCgKrA6ZGTVqlGk8efJkZU5QUFCB1SQicssttxTo/QFf5vznpGEYypyPPvpIyTio4ubxxAoAAAAANNFYAQAAAIAmGisAAAAA0ERjBQAAAACaOLzCTXr06OHpEgCP69u3r5LNnz9fyaw2zjofVCEicuDAAdM4IiJCmWOVPfjgg0pWo0YNJXPe8J6enq7MefLJJ5UMwP+ZMWOGaZySkqLMCQ0NtXUvqwNwZs2aZRqXLVvWfnEAZNu2baax1Z+bzutMROTixYtK9sUXX7ivMD/EEysAAAAA0ERjBQAAAACaaKwAAAAAQBONFQAAAABochhWu8itJjocBV2LVxo+fLiSTZs2TcmysrKUzO5mXV9m85cPCoEn1mifPn1M48WLFytzcnJylCwjI0PJHn30USU7e/asaWy19jp27JhfmSJi/fVx/vVr9ev5+PHjShYZGalkqamptuoobKxR71JU/yy1y+rrM2HCBNN4/Pjxyhyr9RcVFaVkhw4dcr24AsQ69R7eukZbt26tZD/++KOSXb58WckqVKhgGr/wwgvKnHHjxinZ+fPnbdWRnJysZP7G7hrliRUAAAAAaKKxAgAAAABNNFYAAAAAoIkXBOfj8OHDtuYFBAQoWVhYmGnsrd/bDbjqmWeeMY2t1ssrr7yiZAsWLHDp855//nklmzdvnpK1bdvWpftbfW99YmKiknnrfirA15UsWVLJrPZUObty5YqSXb161S01AQXN+WX1a9asUebUrFlTyazOAbDa63zmzBnT2OplwFZ7rMqUKaNkzvu1YMYTKwAAAADQRGMFAAAAAJporAAAAABAE40VAAAAAGji8Ip8WL3c1IrVpvfAwEB3lwN4lVWrVpnGK1asUOb89ttvbvu8SpUqKVmjRo1sXduvXz8lS0pKyve6I0eO2Lo/AH1Wh93Y8d577ykZaxe+YufOnaZx2bJllTmjR49WMquDKuyIjY21Ne/LL79UMjt/bhZlPLECAAAAAE00VgAAAACgicYKAAAAADTRWAEAAACAJodhGIatiRaHMxRVe/bsUbIGDRoo2dy5c03jZ599tsBq8hSbv3xQCPxxjZYrV840ttrYbrWuUlNTlax+/fruK8yHsEa9iyfWacWKFU3jBQsWKHM++ugjW5k73XLLLUqWnJysZFYb+Z3dfvvtSvbrr7+6VpgHsE69hyfW6JgxY0zjsWPHKnOCg4Ndvn9KSoppXK9ePWXOoUOHlOzhhx9WMueDNooKu2uUJ1YAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQVMLTBfii9evXK1mNGjWU7J///GdhlAP4LeeDKYYOHarMOXnypJJ16tSpwGoCfM2MGTNM4wceeECZY3W4y++//65kR48eVbIDBw6Yxi1atLB1/1GjRimZnYMqpk2bpmRWtQK+YuLEiabxlStXlDnNmzdXss6dO9u6f/ny5U3jzz77TJkTFxenZM5rG/njiRUAAAAAaKKxAgAAAABNNFYAAAAAoIk9Vm5i9eKwy5cve6ASwDeFhYUp2eDBg01jq3X2zjvvKNmRI0fcVxjg42bOnGka165dW5nTtm1bJdu4caOSpaWlKdmePXtM4w4dOihzQkJC8qnyT1Zr3PmlwfHx8cqcS5cu2bo/4AumTp3q6RLgIp5YAQAAAIAmGisAAAAA0ERjBQAAAACaaKwAAAAAQBOHV7iJ1UsNH3zwQdM4ISGhsMoBfM4XX3yhZM4HWixevFiZY7WRHcD/+f77703jLVu2KHM+/PBDJZs9e7aS1apVy1bmqrNnzypZeHi42+4PAAWJJ1YAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQxOEVLujdu7eSZWdnK9nevXsLoxzALyxYsEDJXn75ZdN41apVhVUO4LdGjBihZIGBgUpWpkwZW/dr3ry5adyvXz9b12VmZipZly5dbF0LAN6IJ1YAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQ5DAMw7A10eEo6Fp8xtKlS5WsYcOGStajRw/T+NChQwVWk6fY/OWDQsAahRXWqHdhncIK69R7sEZhxe4a5YkVAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANHF4BbSw4dZ7sEZhhTXqXVinsMI69R6sUVjh8AoAAAAAKCQ0VgAAAACgicYKAAAAADTRWAEAAACAJhorAAAAANBEYwUAAAAAmmisAAAAAEATjRUAAAAAaKKxAgAAAABNNFYAAAAAoInGCgAAAAA00VgBAAAAgCYaKwAAAADQ5DAMw/B0EQAAAADgy3hiBQAAAACaaKwAAAAAQBONFQAAAABoorECAAAAAE00VgAAAACgicYKAAAAADTRWAEAAACAJhorAAAAANBEYwUAAAAAmmisAAAAAEATjRUAAAAAaKKxAgAAAABNNFYAAAAAoInGCgAAAAA0FfnGKi0tTRwOh0ydOtVt99y4caM4HA7ZuHGj2+4JFFWsUcD7sU4B78YaLRw+2Vh98MEH4nA4ZMeOHZ4upUAkJCRIdHS0VK9eXQIDA+XWW2+VXr16SVJSkqdLA2zx9zV6zccffyxt27aV0qVLS2hoqLRr1042bNjg6bIAW/x9ne7bt0+GDx8u7dq1k6CgIHE4HJKWlubpsgDb/H2N1qpVSxwOh+WPevXqebo8l5TwdAFQ/fzzz1K+fHmJjY2VSpUqyfHjx+X999+XVq1ayZYtW6Rp06aeLhEo8iZMmCAvvfSS9OrVSwYNGiRXrlyRpKQkOXr0qKdLAyAiW7ZskRkzZkh4eLg0bNhQdu3a5emSAPzF9OnT5fz586bs0KFDMnbsWLn33ns9VJUeGisvNH78eCUbPHiw3HrrrTJnzhyZO3euB6oCcM33338vL730kkybNk2GDx/u6XIAWOjRo4dkZGRISEiITJ06lcYK8DIxMTFK9sorr4iISP/+/Qu5GvfwyW8FtOPy5csyfvx4adGihZQrV05Kly4tHTp0kMTExOte8+abb0pYWJgEBwdLx44dLb/1Ljk5WXr16iUVKlSQoKAgiYiIkNWrV+dbz4ULFyQ5OVlOnTrl0s+nSpUqUqpUKcnIyHDpesDb+PIanT59ulSrVk1iY2PFMAzlX9wAf+HL67RChQoSEhKS7zzAl/nyGrXy3//+V2rXri3t2rVz6XpP89vGKisrS+bPny+RkZEyefJkmTBhgqSnp0t0dLTlv1otWrRIZsyYIc8995yMGTNGkpKSpFOnTnLixIm8Ob/88ou0adNG9u7dKy+++KJMmzZNSpcuLTExMZKQkHDDerZt2yYNGzaUWbNm2f45ZGRkSHp6uvz8888yePBgycrKkqioKNvXA97Ml9foV199JS1btpQZM2ZI5cqVJSQkRG655ZabWt+AL/DldQoUBf60Rn/88UfZu3evPProozd9rdcwfNCCBQsMETG2b99+3Tk5OTlGdna2KTt79qxRtWpV48knn8zLDh48aIiIERwcbBw5ciQv37p1qyEixvDhw/OyqKgoo3HjxsalS5fystzcXKNdu3ZGvXr18rLExERDRIzExEQli4+Pt/3zvOOOOwwRMUTEKFOmjDF27Fjj6tWrtq8HPMWf1+iZM2cMETEqVqxolClTxpgyZYrx8ccfG127djVExJg7d+4Nrwe8hT+vU2dTpkwxRMQ4ePDgTV0HeFJRWqOGYRgjRowwRMTYs2fPTV/rLfz2iVXx4sWlZMmSIiKSm5srZ86ckZycHImIiJCdO3cq82NiYqRGjRp541atWknr1q1l7dq1IiJy5swZ2bBhg/Tu3VvOnTsnp06dklOnTsnp06clOjpaUlJSbrhpPTIyUgzDkAkTJtj+OSxYsEDWrVsns2fPloYNG8rFixfl6tWrtq8HvJmvrtFr3/Z3+vRpmT9/vsTFxUnv3r3ls88+k/Dw8LzvDwf8ga+uU6Co8Jc1mpubK0uXLpXmzZtLw4YNb+pab+LXh1csXLhQpk2bJsnJyXLlypW8vHbt2spcq2Md69evL8uWLRMRkQMHDohhGDJu3DgZN26c5eedPHnS9ItVV9u2bfP+d9++ffN+obnzHQSAJ/niGg0ODhYRkYCAAOnVq1deXqxYMenTp4/Ex8fL4cOHpWbNmlqfA3gLX1ynQFHiD2v066+/lqNHj/r8gVB+21gtXrxYBg0aJDExMTJy5EipUqWKFC9eXCZOnCipqak3fb/c3FwREYmLi5Po6GjLOXXr1tWq+UbKly8vnTp1kiVLltBYwS/46hq9tpE3NDRUihcvbvpvVapUERGRs2fP0ljBL/jqOgWKCn9Zo0uWLJFixYpJv3793H7vwuS3jdXy5culTp06smLFCnE4HHl5fHy85fyUlBQl279/v9SqVUtEROrUqSMif/4rdefOnd1fsA0XL16UzMxMj3w24G6+ukaLFSsmzZo1k+3bt8vly5fzvgVDROT3338XEZHKlSsX2OcDhclX1ylQVPjDGs3OzpZPPvlEIiMjpXr16oXymQXFr/dYiYgYhpGXbd26VbZs2WI5f+XKlabvGd22bZts3bpV7rvvPhH581+iIyMjZd68eXLs2DHl+vT09BvWczPHT548eVLJ0tLS5KuvvpKIiIh8rwd8gS+v0T59+sjVq1dl4cKFedmlS5dkyZIlEh4e7vN/MADX+PI6BYoCf1ija9eulYyMDJ99d9Vf+fQTq/fff1/WrVun5LGxsdK9e3dZsWKF9OzZU+6//345ePCgzJ07V8LDwy3fOVO3bl1p3769DB06VLKzs2X69OlSsWJFGTVqVN6ct99+W9q3by+NGzeWIUOGSJ06deTEiROyZcsWOXLkiOzevfu6tW7btk3uueceiY+Pz3dDX+PGjSUqKkqaNWsm5cuXl5SUFHnvvffkypUrMmnSJPtfIMDD/HWNPvPMMzJ//nx57rnnZP/+/VKzZk358MMP5dChQ/Lpp5/a/wIBXsBf12lmZqbMnDlTREQ2b94sIiKzZs2S0NBQCQ0NlWHDhtn58gAe569r9JolS5ZIYGCgPPzww7bmezXPHEao59rxk9f78dtvvxm5ubnGa6+9ZoSFhRmBgYFG8+bNjTVr1hiPP/64ERYWlneva8dPTpkyxZg2bZpx2223GYGBgUaHDh2M3bt3K5+dmppqDBw40KhWrZoREBBg1KhRw+jevbuxfPnyvDm6x0/Gx8cbERERRvny5Y0SJUoY1atXN/r27Wv89NNPOl82oND4+xo1DMM4ceKE8fjjjxsVKlQwAgMDjdatWxvr1q1z9UsGFDp/X6fXarL68dfaAW/l72vUMAwjMzPTCAoKMh566CFXv0xexWEYf3l2CAAAAAC4aX67xwoAAAAACguNFQAAAABoorECAAAAAE00VgAAAACgicYKAAAAADTRWAEAAACAJhorAAAAANBUwu5Eh8NRkHXAR/EaNO/BGoUV1qh3YZ3CCuvUe7BGYcXuGuWJFQAAAABoorECAAAAAE00VgAAAACgicYKAAAAADTRWAEAAACAJhorAAAAANBEYwUAAAAAmmisAAAAAEATjRUAAAAAaCrh6QIAwFPq169vGq9bt06ZU7x4cSULCwsrsJoAAIBv4okVAAAAAGiisQIAAAAATTRWAAAAAKCJPVYAioSZM2cqWZ8+fUzjChUqKHPWrFlTYDUBAAD/wRMrAAAAANBEYwUAAAAAmmisAAAAAEATjRUAAAAAaHIYhmHYmuhwFHQt8EE2f/mgEBTVNVq1alUlW7FihZK1adNGyZx//SYlJSlzoqKilOz06dM3U6JHsUa9S1Fdp7gx1qn3YI3Cit01yhMrAAAAANBEYwUAAAAAmmisAAAAAEATjRUAAAAAaCpR0B9QpkwZJevTp4+SXbp0SclatGhhGoeEhChz+vfvr2QbN25UsqNHj96ozJty/PhxJVu1apWS7dixw22fCUCkfv36SjZ16lQla926ta37jRkzxjS2WrO+dFAF4AnOm/0/+ugjZU63bt2ULDw8XMmOHDnivsIAoJDxxAoAAAAANNFYAQAAAIAmGisAAAAA0ERjBQAAAACaHIbNVwm7+ibq119/Xcni4uJcupc3y83NVbI9e/aYxlYbeq2ytLQ0t9VV0HhbvPcoCm+Lb9OmjZJt2rTJ1rVWX58BAwaYxlbr0dexRr2LP67TUqVKmcb79u1T5tSoUUPJnn76aSWbP3+++wrzIaxT7+GPaxT67K5RnlgBAAAAgCYaKwAAAADQRGMFAAAAAJoK/AXBDz30kNvuZfWizp9++slt97f6vvA77rhDyUJDQ5WsefPmStaoUSPT+NVXX1XmWNXvS3usgILk/ELg//73v8ocu98Pb/V7kdWLvQHcnAsXLpjGKSkpyhyrPVaVK1cusJoAFIwRI0YoWcmSJU3jhg0bKnP69+9v6/7Jycmm8Z133nkT1XkeT6wAAAAAQBONFQAAAABoorECAAAAAE00VgAAAACgqcAPr4iOjlYy5w3pIiL79+/P917OG2RFRI4dO+ZaYRpCQkKU7Oeff1aymjVr5nuvHj16KNlnn33mWmGAn3nsscdMY6s1tXbtWiX7+9//rmRHjx51X2EAruvtt99WssjISCWz2uAOoOB17NhRyZwPXLvevJ49eyqZnUOk7L5gt169eqbxnj17lDnh4eG27uUJPLECAAAAAE00VgAAAACgicYKAAAAADTRWAEAAACAJodhczeZnY1pRUW/fv2UbMmSJflel52drWQdOnRQsh07drhWmAfY3YyIgufra/S7775TsmbNmpnGv//+uzKna9euSnbgwAG31eXrWKPexdfXqR233Xabkh06dEjJLl++rGS1a9dWMk8cUlXYWKfew1vX6C233KJkH330kZLVqVMn33uVK1dOyUqXLq1kVl+LH374QcnuuuuufD/TVVYHT4WFhRXY512P3TXKEysAAAAA0ERjBQAAAACaaKwAAAAAQBONFQAAAABoKuHpArxJyZIllWzGjBlKNnDgQJfu37ZtWyXbtWuXS/cCfNmDDz6oZK1bt1Yy582i/+///T9lzqVLl9xXGIACYbUJ3urP3B49eijZvHnzCqQmwFt17txZyd59910lszooxp3Cw8OV7NSpU0pWqVIl07h69erKnAULFijZrbfemm8Ne/bsyXeON+GJFQAAAABoorECAAAAAE00VgAAAACgicYKAAAAADQV6cMr7rnnHtP4scceU+YMGjTI1r2uXLmiZC+88IJpnJycbL84wE+EhoYqWYcOHVy619mzZ5XsyJEjLt3LSmxsrJLZ3RwcFxfntjoAf+N8EM31WB1oARQ1o0aNUjKdgyqys7NN49GjRytzvv/+eyXbt2+frfufPn3aNLb6s9TOQRUiImlpaaax1d/NvRlPrAAAAABAE40VAAAAAGiisQIAAAAATUVmj1WrVq2UbP369aZx8eLFXb6/1fePHz582DS+evWqy/cHfJXVr/sWLVooWbFi6r/z5ObmmsbffPONy3UMHz483znPP/+8koWFhdm6/4gRI0xjq+8nP3r0qK17AQCKjnvvvdc0btOmjcv3cv67p4i6T2nz5s0u398Ou/uprKxatco0tnohsTfjiRUAAAAAaKKxAgAAAABNNFYAAAAAoInGCgAAAAA0FZnDK3r37q1kOodVOLN6qeFnn31mGu/YsUOZ8+mnnypZQkKCkiUlJWlUB3hOx44dlczqBcHOB1WIqJtw7W5ibdasma3P7NGjR773+uOPP5TM6qXEd9xxh2m8fPlyZU7fvn2V7NChQ/nWAADwX86HH5UqVcrWdd99952S/ec//1Eydx5WUb58eSXr2rWraXz33XfbupdV/WvXrnWtMC/BEysAAAAA0ERjBQAAAACaaKwAAAAAQBONFQAAAABoKjKHV6xYsULJGjZsaBq3bNlSmVOpUiW31RAREWEri4+PV7Lp06ebxq+//roy5+TJk64XB7hBSEiIktWuXdvWtb///ruSffjhh6bxgQMHlDn169dXspEjRyrZgw8+qGTOh2GsX79emTNt2jQlK1eunJJt2LAh3zlAUeVwOJTMMAwPVAJ4n3feecc0tvq7Z2ZmppI9+uijSnb8+HH3FWbh73//u5K9/PLL+V73yy+/KJnVwXIFXX9B44kVAAAAAGiisQIAAAAATTRWAAAAAKCJxgoAAAAANBWZwyus3u58//33m8Y1a9ZU5lhtIKxataqSPfTQQ0r25JNPmsZWm3etFCum9rv//Oc/TeMWLVooc6KiopQsNzfX1mcC7tC+fXsle/PNN21d++677yrZSy+9ZBpbrb2pU6cqWbdu3ZTs3LlzSrZs2TLTOC4uTplTr149JZs7d26+9//qq6+UOYcOHVIyoCjgoArg+j755JMbjj3lgQceULLx48fne11OTo6SWf256esHVVjhiRUAAAAAaKKxAgAAAABNNFYAAAAAoInGCgAAAAA0OQybO0rtHryA/9O/f3/T+Pnnn1fmtGrVym2f9+KLLyrZ66+/7rb7W2FDsvfwhjU6evRoJXv11VdtXVuiRP5n6WzevFnJWrdubev+Voe7fP3116ZxmzZtlDmbNm2ydf/p06ebxlYHYXgCa9S7eMM6LWi33Xabktk9uOWee+5RMud16o9Yp96jKKxRu65evapkdn6tPvvss0r2zjvvuKUmT7G7RnliBQAAAACaaKwAAAAAQBONFQAAAABoKjIvCPaEJUuWmMYff/yxMufLL79Usrvvvtulz6tbt65L1wHuEhoaqmRW36++atUqW/dr1qyZaVyrVi1b9x8xYoSSWe3TqF+/vmn83//+1+X7O++xAnDzUlNTPV0CUCS99tprSlasmPr8JTc3N997FYV9kdfDEysAAAAA0ERjBQAAAACaaKwAAAAAQBONFQAAAABo4vCKQpSTk6NkP/zwg5K5enjF/v37XboOKEhWL9Vz9WWYVptmre7VpEkTJTt8+LCSBQUFmcYHDx5U5nTo0EHJMjMzb1gnAADeqmTJkkrWvHlzJbP7Z25sbKxpnJKSolGdb+OJFQAAAABoorECAAAAAE00VgAAAACgicYKAAAAADT5/OEVt9xyi5INGTJEyZKTk5Vs2bJlBVLT9RQvXlzJmjZt6tK9rA7C+P777126F+Auq1atUrKRI0cq2YMPPqhkbdq0UbJmzZqZxiEhIbbqGDhwoJI5HA4lO3XqlGk8YcIEZc7Ro0dtfSYAfYGBgZ4uAfA7pUqVMo0HDBigzOnSpYute3300UdKtmTJEtPY6tCLooInVgAAAACgicYKAAAAADTRWAEAAACAJhorAAAAANDkU4dXVKtWTcnWrVunZI0bN1ay8uXLF0hNN1K1alXT+J///Kcyp1OnTi7de+/evUq2adMml+4FuMuVK1eU7MKFC0rmvJFWRGTz5s1KZvWGd1edO3dOyZwPsPnf//7nts8DcPO6deumZDNnzvRAJYBvsjrk6d133zWNe/XqZetew4cPV7JZs2YpWVE+rMIZT6wAAAAAQBONFQAAAABoorECAAAAAE0+tcdq+vTpSma1n8pK7dq1lWzfvn2m8cWLF23dKzg4WMlGjRqlZM57quy+3NTqRabO+0NeeOEFW/cCCtMPP/ygZP369VMyq/2GkZGRLn3mwoULleznn39Wsh9//FHJvv76a5c+E8D1nThxQsl++eUXJbvzzjsLoxygSKlRo4aS2dlTlZqaqmQzZsxwS01FCU+sAAAAAEATjRUAAAAAaKKxAgAAAABNNFYAAAAAoMmnDq/46quvlKx37962rt25c6eSOW9mz8zMtHWvcuXKKVnz5s1tXWuH1YtMe/bsaRqz6R6+4rPPPrOVAfAPly9fVrJLly7ZurZLly5KxguCAWsNGjRQshEjRuR73f79+5Xsvvvuc0tNRR1PrAAAAABAE40VAAAAAGiisQIAAAAATTRWAAAAAKDJpw6v+OKLL5Rs6dKlSta3b19b93PngRN25OTkKNn06dOV7JNPPlGyrVu3FkRJAAAUuF27dilZixYtlKxMmTKFUA3gH8aNG6dkffr0yfc6qwNhDh065JaaijqeWAEAAACAJhorAAAAANBEYwUAAAAAmmisAAAAAECTTx1ekZaWpmRPPPGEkq1evVrJOnXqpGTOb57u0aOHrTqSk5NtzduwYUO+11lt6AUAwJ+8+uqrStaoUSMlW7ZsWWGUA/icO++8U8nKli1r69p33nnHNHb++ynchydWAAAAAKCJxgoAAAAANNFYAQAAAIAmGisAAAAA0OQwDMOwNdHhKOha4INs/vJBIWCNwgpr1LuwTmGFdeo9vHWNTp48WclGjBihZIcOHVKybt26mcb79u1zX2FFhN01yhMrAAAAANBEYwUAAAAAmmisAAAAAEATe6yghe8L9x6sUVhhjXoX1imssE69h7eu0aioKCX7/PPPlezhhx9WslWrVhVITUUJe6wAAAAAoJDQWAEAAACAJhorAAAAANBEYwUAAAAAmji8AlrYcOs9WKOwwhr1LqxTWGGdeg/WKKxweAUAAAAAFBIaKwAAAADQRGMFAAAAAJporAAAAABAk+3DKwAAAAAA1nhiBQAAAACaaKwAAAAAQBONFQAAAABoorECAAAAAE00VgAAAACgicYKAAAAADTRWAEAAACAJhorAAAAANBEYwUAAAAAmv4/IhzF9lPu548AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 7))   #displaying images with labels from train set images\n",
        "\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(train_images[i], cmap='gray')\n",
        "    plt.title(f\"Label: {train_labels[i]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "vS3bVUJg8AXR",
        "outputId": "9ef52813-af48-41f6-cbd1-70c818e16815"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  84, 185, 159, 151,  60,  36,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0, 222, 254, 254, 254, 254, 241, 198,\n",
              "        198, 198, 198, 198, 198, 198, 198, 170,  52,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  67, 114,  72, 114, 163, 227, 254,\n",
              "        225, 254, 254, 254, 250, 229, 254, 254, 140,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66,\n",
              "         14,  67,  67,  67,  59,  21, 236, 254, 106,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,  83, 253, 209,  18,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,  22, 233, 255,  83,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 129, 254, 238,  44,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  59, 249, 254,  62,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0, 133, 254, 187,   5,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   9, 205, 248,  58,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0, 126, 254, 182,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  75, 251, 240,  57,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         19, 221, 254, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "        203, 254, 219,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,\n",
              "        254, 254,  77,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 224,\n",
              "        254, 115,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 133, 254,\n",
              "        254,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  61, 242, 254,\n",
              "        254,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 254,\n",
              "        219,  40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 207,\n",
              "         18,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-5f3a2f02-0e86-47da-ac20-5a99a7489db2\" class=\"ndarray_repr\"><pre>ndarray (28, 28) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  84, 185, 159, 151,  60,  36,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0, 222, 254, 254, 254, 254, 241, 198,\n",
              "        198, 198, 198, 198, 198, 198, 198, 170,  52,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  67, 114,  72, 114, 163, 227, 254,\n",
              "        225, 254, 254, 254, 250, 229, 254, 254, 140,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66,\n",
              "         14,  67,  67,  67,  59,  21, 236, 254, 106,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,  83, 253, 209,  18,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,  22, 233, 255,  83,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 129, 254, 238,  44,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  59, 249, 254,  62,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0, 133, 254, 187,   5,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   9, 205, 248,  58,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0, 126, 254, 182,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  75, 251, 240,  57,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         19, 221, 254, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "        203, 254, 219,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,\n",
              "        254, 254,  77,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 224,\n",
              "        254, 115,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 133, 254,\n",
              "        254,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  61, 242, 254,\n",
              "        254,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 254,\n",
              "        219,  40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 207,\n",
              "         18,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-5f3a2f02-0e86-47da-ac20-5a99a7489db2 button').onclick = (e) => {\n",
              "        document.querySelector('#id-5f3a2f02-0e86-47da-ac20-5a99a7489db2').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-5f3a2f02-0e86-47da-ac20-5a99a7489db2 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "test_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "bW5TSRh08Ecw",
        "outputId": "fb5605f1-3e0d-40dc-df08-025e4c6f9693"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-6184eeab-2024-4bda-93c1-09489d9c0168\" class=\"ndarray_repr\"><pre>ndarray (28, 28) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-6184eeab-2024-4bda-93c1-09489d9c0168 button').onclick = (e) => {\n",
              "        document.querySelector('#id-6184eeab-2024-4bda-93c1-09489d9c0168').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-6184eeab-2024-4bda-93c1-09489d9c0168 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuXEkSPM8KZR",
        "outputId": "13950cae-6901-4ce8-cc9b-86898cf85ba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: (48000, 28, 28, 1)\n",
            "Training labels: (48000,)\n",
            "Testing images: (12000, 28, 28, 1)\n",
            "Testing labels: (12000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Reshape images to have a single channel\n",
        "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "\n",
        "#80:20 split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training images:\", train_images.shape)\n",
        "print(\"Training labels:\", train_labels.shape)\n",
        "print(\"Testing images:\", test_images.shape)\n",
        "print(\"Testing labels:\", test_labels.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEtOrSRE8WaR",
        "outputId": "aa8751ef-6fa1-4c69-b933-b2c019123b6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.06666667],\n",
              "        [0.18431373],\n",
              "        [0.18431373],\n",
              "        [0.18431373],\n",
              "        [0.18431373],\n",
              "        [0.18431373],\n",
              "        [0.18431373],\n",
              "        [0.10980392],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.05490196],\n",
              "        [0.24705882],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.24313726],\n",
              "        [0.5411765 ],\n",
              "        [0.77254903],\n",
              "        [0.77254903],\n",
              "        [0.77254903],\n",
              "        [0.8509804 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.9019608 ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.5921569 ],\n",
              "        [0.6666667 ],\n",
              "        [0.        ],\n",
              "        [0.21960784],\n",
              "        [0.9098039 ],\n",
              "        [0.98039216],\n",
              "        [0.99215686],\n",
              "        [0.90588236],\n",
              "        [0.8509804 ],\n",
              "        [0.8509804 ],\n",
              "        [0.8509804 ],\n",
              "        [0.8509804 ],\n",
              "        [0.8509804 ],\n",
              "        [0.8509804 ],\n",
              "        [0.8509804 ],\n",
              "        [0.5019608 ],\n",
              "        [0.7019608 ],\n",
              "        [0.5176471 ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.40784314],\n",
              "        [0.9843137 ],\n",
              "        [0.6666667 ],\n",
              "        [0.07843138],\n",
              "        [0.9137255 ],\n",
              "        [0.99215686],\n",
              "        [0.78039217],\n",
              "        [0.32941177],\n",
              "        [0.10196079],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.01960784],\n",
              "        [0.79607844],\n",
              "        [0.99215686],\n",
              "        [0.627451  ],\n",
              "        [0.01176471],\n",
              "        [0.07843138],\n",
              "        [0.07843138],\n",
              "        [0.02745098],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.3019608 ],\n",
              "        [0.99215686],\n",
              "        [0.89411765],\n",
              "        [0.08235294],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.5882353 ],\n",
              "        [0.99215686],\n",
              "        [0.9764706 ],\n",
              "        [0.8745098 ],\n",
              "        [0.8745098 ],\n",
              "        [0.75686276],\n",
              "        [0.2784314 ],\n",
              "        [0.02352941],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.49411765],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.76862746],\n",
              "        [0.4745098 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.03137255],\n",
              "        [0.7921569 ],\n",
              "        [0.7490196 ],\n",
              "        [0.5647059 ],\n",
              "        [0.5647059 ],\n",
              "        [0.8509804 ],\n",
              "        [0.9843137 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.6666667 ],\n",
              "        [0.01568628],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.20784314],\n",
              "        [0.9254902 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.84313726],\n",
              "        [0.01176471],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.13333334],\n",
              "        [0.7058824 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.5019608 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00784314],\n",
              "        [0.54901963],\n",
              "        [0.99215686],\n",
              "        [0.9254902 ],\n",
              "        [0.14117648],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.14509805],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.05098039],\n",
              "        [0.84313726],\n",
              "        [0.99215686],\n",
              "        [0.24313726],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.9647059 ],\n",
              "        [0.20784314],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.4117647 ],\n",
              "        [0.99215686],\n",
              "        [0.24313726],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.99607843],\n",
              "        [0.22352941],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.3882353 ],\n",
              "        [0.99215686],\n",
              "        [0.24313726],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [1.        ],\n",
              "        [0.25490198],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.13725491],\n",
              "        [0.8509804 ],\n",
              "        [0.99215686],\n",
              "        [0.24313726],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.8901961 ],\n",
              "        [0.8745098 ],\n",
              "        [0.4509804 ],\n",
              "        [0.07058824],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.01960784],\n",
              "        [0.26666668],\n",
              "        [0.44313726],\n",
              "        [0.87058824],\n",
              "        [0.99215686],\n",
              "        [0.9490196 ],\n",
              "        [0.18039216],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.16078432],\n",
              "        [0.8784314 ],\n",
              "        [0.99215686],\n",
              "        [0.8901961 ],\n",
              "        [0.85490197],\n",
              "        [0.85490197],\n",
              "        [0.8627451 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.9764706 ],\n",
              "        [0.29803923],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.13725491],\n",
              "        [0.5882353 ],\n",
              "        [0.84313726],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.8039216 ],\n",
              "        [0.6862745 ],\n",
              "        [0.22352941],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.0627451 ],\n",
              "        [0.18039216],\n",
              "        [0.18039216],\n",
              "        [0.18039216],\n",
              "        [0.18039216],\n",
              "        [0.03137255],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRxjh4Q69Ely",
        "outputId": "940ba51e-a8a9-4d45-bd9e-582faa0a376d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.9224166666666667\n",
            "SVM Accuracy: 0.9775833333333334\n",
            "Random Forest Accuracy: 0.9675833333333334\n",
            "knn Accuracy: 0.9715\n",
            "decision_tree Accuracy: 0.8675833333333334\n",
            "xgb Accuracy: 0.977\n",
            "naive_bayes Accuracy: 0.5600833333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#gradient boosting\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize models\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "svm_classifier = SVC()\n",
        "random_forest = RandomForestClassifier()\n",
        "knn = KNeighborsClassifier()\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "xgb = XGBClassifier()\n",
        "naive_bayes = GaussianNB()\n",
        "\n",
        "# Fit the models\n",
        "logistic_regression.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "svm_classifier.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "random_forest.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "knn.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "decision_tree.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "xgb.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "naive_bayes.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "\n",
        "# Predictions\n",
        "logistic_regression_pred = logistic_regression.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "svm_pred = svm_classifier.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "random_forest_pred = random_forest.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "knn_pred = knn.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "decision_tree_pred = decision_tree.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "xgb_pred = xgb.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "naive_bayes_pred = naive_bayes.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "\n",
        "# Accuracy\n",
        "logistic_regression_accuracy = accuracy_score(test_labels, logistic_regression_pred)\n",
        "svm_accuracy = accuracy_score(test_labels, svm_pred)\n",
        "random_forest_accuracy = accuracy_score(test_labels, random_forest_pred)\n",
        "knn_accuracy = accuracy_score(test_labels, knn_pred)\n",
        "decision_tree_accuracy = accuracy_score(test_labels, decision_tree_pred)\n",
        "xgb_accuracy = accuracy_score(test_labels, xgb_pred)\n",
        "naive_bayes_accuracy = accuracy_score(test_labels, naive_bayes_pred)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", logistic_regression_accuracy)\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"Random Forest Accuracy:\", random_forest_accuracy)\n",
        "print(\"knn Accuracy:\", knn_accuracy)\n",
        "print(\"decision_tree Accuracy:\", decision_tree_accuracy)\n",
        "print(\"xgb Accuracy:\", xgb_accuracy)\n",
        "print(\"naive_bayes Accuracy:\", naive_bayes_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " For, **80:20 split**, As can be seen from above results, SVM has shown best accuracy as compared to other models."
      ],
      "metadata": {
        "id": "JsPCL0DrfBNJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edTjN7mHCOuc",
        "outputId": "948a6425-507c-428f-bb47-fe1386c3319e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation metrics for Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96      1175\n",
            "           1       0.95      0.97      0.96      1322\n",
            "           2       0.90      0.90      0.90      1174\n",
            "           3       0.91      0.89      0.90      1219\n",
            "           4       0.93      0.94      0.94      1176\n",
            "           5       0.89      0.88      0.89      1104\n",
            "           6       0.95      0.95      0.95      1177\n",
            "           7       0.94      0.93      0.93      1299\n",
            "           8       0.90      0.87      0.88      1160\n",
            "           9       0.90      0.91      0.91      1194\n",
            "\n",
            "    accuracy                           0.92     12000\n",
            "   macro avg       0.92      0.92      0.92     12000\n",
            "weighted avg       0.92      0.92      0.92     12000\n",
            "\n",
            "Confusion Matrix for Logistic Regression\n",
            "[[1138    0    7    1    3    7    7    4    6    2]\n",
            " [   0 1281   10    3    2    5    1    3   14    3]\n",
            " [   3   19 1052   16   14    7   17   16   24    6]\n",
            " [   4    8   34 1088    1   35    2   11   21   15]\n",
            " [   1    2    5    2 1106    2   10    3    7   38]\n",
            " [  15    9   14   29    9  977   11    4   29    7]\n",
            " [   9    2   16    1    9   17 1117    2    4    0]\n",
            " [   5    8   17    8    9    2    0 1207    5   38]\n",
            " [  10   16   14   34    8   35   12    4 1014   13]\n",
            " [   5    5    6   10   24   11    0   36    8 1089]]\n",
            "\n",
            "\n",
            "Evaluation metrics for SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1175\n",
            "           1       0.99      0.99      0.99      1322\n",
            "           2       0.96      0.98      0.97      1174\n",
            "           3       0.98      0.96      0.97      1219\n",
            "           4       0.97      0.98      0.98      1176\n",
            "           5       0.98      0.97      0.97      1104\n",
            "           6       0.99      0.99      0.99      1177\n",
            "           7       0.98      0.97      0.98      1299\n",
            "           8       0.97      0.97      0.97      1160\n",
            "           9       0.97      0.97      0.97      1194\n",
            "\n",
            "    accuracy                           0.98     12000\n",
            "   macro avg       0.98      0.98      0.98     12000\n",
            "weighted avg       0.98      0.98      0.98     12000\n",
            "\n",
            "Confusion Matrix for SVM\n",
            "[[1163    0    3    0    2    3    1    0    2    1]\n",
            " [   0 1310    5    2    1    0    0    0    1    3]\n",
            " [   1    5 1149    2    5    1    0    6    4    1]\n",
            " [   1    1   15 1169    0   11    0    4   13    5]\n",
            " [   2    0    2    0 1157    0    1    2    0   12]\n",
            " [   3    1    3   12    4 1069    5    1    5    1]\n",
            " [   1    1    1    0    1    3 1166    0    4    0]\n",
            " [   1    7   12    3    6    0    0 1263    3    4]\n",
            " [   1    2    4    6    6    6    2    2 1128    3]\n",
            " [   4    2    1    1   14    3    0    8    4 1157]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98      1175\n",
            "           1       0.98      0.99      0.98      1322\n",
            "           2       0.95      0.97      0.96      1174\n",
            "           3       0.96      0.95      0.96      1219\n",
            "           4       0.96      0.97      0.97      1176\n",
            "           5       0.96      0.96      0.96      1104\n",
            "           6       0.98      0.98      0.98      1177\n",
            "           7       0.98      0.96      0.97      1299\n",
            "           8       0.96      0.96      0.96      1160\n",
            "           9       0.96      0.95      0.95      1194\n",
            "\n",
            "    accuracy                           0.97     12000\n",
            "   macro avg       0.97      0.97      0.97     12000\n",
            "weighted avg       0.97      0.97      0.97     12000\n",
            "\n",
            "Confusion Matrix for Random Forest\n",
            "[[1154    0    2    1    4    3    4    0    7    0]\n",
            " [   0 1305    7    4    3    0    1    1    0    1]\n",
            " [   2    6 1143    5    3    0    2    8    4    1]\n",
            " [   2    0   17 1157    1   15    1    6   12    8]\n",
            " [   2    0    1    2 1140    0    5    3    0   23]\n",
            " [   5    3    2   12    2 1056    7    1   10    6]\n",
            " [   5    0    0    0    4    7 1158    0    3    0]\n",
            " [   1   11   16    2    6    0    0 1251    3    9]\n",
            " [   1    3    6   10    9   11    4    1 1110    5]\n",
            " [   6    2    5    8   12    5    2   10    7 1137]]\n",
            "\n",
            "\n",
            "Evaluation metrics for K-Nearest Neighbors\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99      1175\n",
            "           1       0.95      1.00      0.97      1322\n",
            "           2       0.98      0.96      0.97      1174\n",
            "           3       0.96      0.97      0.97      1219\n",
            "           4       0.98      0.97      0.97      1176\n",
            "           5       0.96      0.97      0.97      1104\n",
            "           6       0.98      0.99      0.99      1177\n",
            "           7       0.97      0.97      0.97      1299\n",
            "           8       0.99      0.93      0.96      1160\n",
            "           9       0.96      0.96      0.96      1194\n",
            "\n",
            "    accuracy                           0.97     12000\n",
            "   macro avg       0.97      0.97      0.97     12000\n",
            "weighted avg       0.97      0.97      0.97     12000\n",
            "\n",
            "Confusion Matrix for K-Nearest Neighbors\n",
            "[[1166    0    1    0    1    2    3    1    0    1]\n",
            " [   0 1317    2    0    0    0    0    2    0    1]\n",
            " [   7   13 1130    2    2    2    1   15    1    1]\n",
            " [   1    0    7 1185    0   10    0    3    6    7]\n",
            " [   0    9    1    1 1135    0    3    2    0   25]\n",
            " [   5    4    0   13    0 1068    9    0    3    2]\n",
            " [   2    3    1    0    1    3 1167    0    0    0]\n",
            " [   0   22    2    0    3    1    0 1264    1    6]\n",
            " [   6   12    5   22    5   19    2    3 1081    5]\n",
            " [   4    4    1    5   16    2    2   15    0 1145]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Decision Trees\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.92      1175\n",
            "           1       0.93      0.96      0.95      1322\n",
            "           2       0.85      0.85      0.85      1174\n",
            "           3       0.84      0.84      0.84      1219\n",
            "           4       0.85      0.87      0.86      1176\n",
            "           5       0.83      0.81      0.82      1104\n",
            "           6       0.90      0.89      0.89      1177\n",
            "           7       0.90      0.90      0.90      1299\n",
            "           8       0.81      0.79      0.80      1160\n",
            "           9       0.81      0.83      0.82      1194\n",
            "\n",
            "    accuracy                           0.87     12000\n",
            "   macro avg       0.87      0.87      0.87     12000\n",
            "weighted avg       0.87      0.87      0.87     12000\n",
            "\n",
            "Confusion Matrix for Decision Trees\n",
            "[[1072    1   19    9    7   25   10    5   14   13]\n",
            " [   0 1271    6    9    3    8    9    3   10    3]\n",
            " [  17   11  998   23   18   14   10   24   42   17]\n",
            " [  14    8   39 1018    6   54    4   18   28   30]\n",
            " [   2    6    9    8 1026    7   13   16   13   76]\n",
            " [  15   13   13   57   11  896   33    8   34   24]\n",
            " [  17   12   18    6   22   26 1043    6   25    2]\n",
            " [   5   15   24   20   15    5    3 1174   10   28]\n",
            " [  11   16   37   51   25   25   28    9  921   37]\n",
            " [   6   10    7   12   68   22    7   36   34  992]]\n",
            "\n",
            "\n",
            "Evaluation metrics for XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99      1175\n",
            "           1       0.99      0.99      0.99      1322\n",
            "           2       0.96      0.98      0.97      1174\n",
            "           3       0.98      0.96      0.97      1219\n",
            "           4       0.97      0.98      0.98      1176\n",
            "           5       0.98      0.98      0.98      1104\n",
            "           6       0.99      0.99      0.99      1177\n",
            "           7       0.98      0.97      0.97      1299\n",
            "           8       0.97      0.98      0.97      1160\n",
            "           9       0.96      0.97      0.97      1194\n",
            "\n",
            "    accuracy                           0.98     12000\n",
            "   macro avg       0.98      0.98      0.98     12000\n",
            "weighted avg       0.98      0.98      0.98     12000\n",
            "\n",
            "Confusion Matrix for XGBoost\n",
            "[[1156    0    2    2    4    0    1    2    8    0]\n",
            " [   0 1307    8    4    1    0    1    0    1    0]\n",
            " [   0    4 1151    2    4    0    0    8    3    2]\n",
            " [   1    0   12 1172    1    8    0    6    9   10]\n",
            " [   2    0    1    1 1149    0    1    3    0   19]\n",
            " [   3    1    2    8    2 1077    2    0    7    2]\n",
            " [   3    0    0    1    2    6 1160    0    5    0]\n",
            " [   0    8   14    1    2    0    0 1263    3    8]\n",
            " [   1    0    7    4    4    4    2    1 1134    3]\n",
            " [   2    3    3    3   11    2    2   11    2 1155]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.87      0.81      1175\n",
            "           1       0.80      0.94      0.86      1322\n",
            "           2       0.84      0.32      0.46      1174\n",
            "           3       0.76      0.38      0.51      1219\n",
            "           4       0.87      0.19      0.31      1176\n",
            "           5       0.62      0.06      0.10      1104\n",
            "           6       0.66      0.95      0.78      1177\n",
            "           7       0.93      0.27      0.42      1299\n",
            "           8       0.28      0.62      0.39      1160\n",
            "           9       0.37      0.96      0.54      1194\n",
            "\n",
            "    accuracy                           0.56     12000\n",
            "   macro avg       0.69      0.56      0.52     12000\n",
            "weighted avg       0.69      0.56      0.52     12000\n",
            "\n",
            "Confusion Matrix for Naive Bayes\n",
            "[[1028    1    9    0    0    2   43    1   68   23]\n",
            " [   2 1237    7    8    0    3   12    1   41   11]\n",
            " [  73   33  371   86    6    7  290    0  295   13]\n",
            " [  58   65   15  466    2    5   55   10  423  120]\n",
            " [  26    8   13    6  225    6  104    2  231  555]\n",
            " [ 124   31    6   22    5   61   52    3  683  117]\n",
            " [   9   18    8    2    1    2 1113    0   23    1]\n",
            " [   9   11    3   15   13    2    0  353   44  849]\n",
            " [  18  145    6    7    3   10   21    1  721  228]\n",
            " [   7    3    4    0    5    1    0    9   19 1146]]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Function to print metrics and confusion matrix\n",
        "def print_evaluation_metrics(y_true, y_pred, model_name):\n",
        "    print(\"Evaluation metrics for\", model_name)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Confusion Matrix for\", model_name)\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Logistic Regression\n",
        "print_evaluation_metrics(test_labels, logistic_regression_pred, \"Logistic Regression\")\n",
        "\n",
        "# SVM\n",
        "print_evaluation_metrics(test_labels, svm_pred, \"SVM\")\n",
        "\n",
        "# Random Forest\n",
        "print_evaluation_metrics(test_labels, random_forest_pred, \"Random Forest\")\n",
        "\n",
        "#knn\n",
        "print_evaluation_metrics(test_labels, knn_pred, \"K-Nearest Neighbors\")\n",
        "\n",
        "#decision\n",
        "print_evaluation_metrics(test_labels, decision_tree_pred, \"Decision Trees\")\n",
        "\n",
        "#gradient boosting\n",
        "print_evaluation_metrics(test_labels, xgb_pred, \"XGBoost\")\n",
        "\n",
        "#naive bayes\n",
        "print_evaluation_metrics(test_labels, naive_bayes_pred, \"Naive Bayes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For, 80:20 split, As can be seen from above results, SVM, Random Forest, XGBoost, and K-Nearest Neighbors demonstrate high accuracy, precision, recall, and F1-score across multiple classes, indicating strong overall performance.\n",
        "Naive Bayes, however, exhibits comparatively lower performance across most metrics, especially precision, recall, and F1-score for several classes"
      ],
      "metadata": {
        "id": "R9ZTc0XGGxhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#50:50 split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(train_images, train_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Training images:\", train_images.shape)\n",
        "print(\"Training labels:\", train_labels.shape)\n",
        "print(\"Testing images:\", test_images.shape)\n",
        "print(\"Testing labels:\", test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zEo35b6J6Jw",
        "outputId": "2fe9fcb7-b11d-4e56-fb85-424a9105c646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: (24000, 28, 28)\n",
            "Training labels: (24000,)\n",
            "Testing images: (24000, 28, 28)\n",
            "Testing labels: (24000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#gradient boosting\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize models\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "svm_classifier = SVC()\n",
        "random_forest = RandomForestClassifier()\n",
        "knn = KNeighborsClassifier()\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "xgb = XGBClassifier()\n",
        "naive_bayes = GaussianNB()\n",
        "\n",
        "# Fit the models\n",
        "\n",
        "\n",
        "logistic_regression.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "svm_classifier.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "random_forest.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "knn.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "decision_tree.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "xgb.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "naive_bayes.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "\n",
        "# Predictions\n",
        "logistic_regression_pred = logistic_regression.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "svm_pred = svm_classifier.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "random_forest_pred = random_forest.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "knn_pred = knn.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "decision_tree_pred = decision_tree.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "xgb_pred = xgb.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "naive_bayes_pred = naive_bayes.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "\n",
        "# Accuracy\n",
        "logistic_regression_accuracy = accuracy_score(test_labels, logistic_regression_pred)\n",
        "svm_accuracy = accuracy_score(test_labels, svm_pred)\n",
        "random_forest_accuracy = accuracy_score(test_labels, random_forest_pred)\n",
        "knn_accuracy = accuracy_score(test_labels, knn_pred)\n",
        "decision_tree_accuracy = accuracy_score(test_labels, decision_tree_pred)\n",
        "xgb_accuracy = accuracy_score(test_labels, xgb_pred)\n",
        "naive_bayes_accuracy = accuracy_score(test_labels, naive_bayes_pred)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", logistic_regression_accuracy)\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"Random Forest Accuracy:\", random_forest_accuracy)\n",
        "print(\"knn Accuracy:\", knn_accuracy)\n",
        "print(\"decision_tree Accuracy:\", decision_tree_accuracy)\n",
        "print(\"xgb Accuracy:\", xgb_accuracy)\n",
        "print(\"naive_bayes Accuracy:\", naive_bayes_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0ZvQTEoKI6l",
        "outputId": "a8991a53-73c7-4246-bcbf-e2fe441a7c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.881625\n",
            "SVM Accuracy: 0.9715416666666666\n",
            "Random Forest Accuracy: 0.96\n",
            "knn Accuracy: 0.963375\n",
            "decision_tree Accuracy: 0.8441666666666666\n",
            "xgb Accuracy: 0.9695833333333334\n",
            "naive_bayes Accuracy: 0.5627083333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For, **50:50 split**, As can be seen from above results, SVM has shown best accuracy as compared to other models."
      ],
      "metadata": {
        "id": "ct1UEbKPOs9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Function to print metrics and confusion matrix\n",
        "def print_evaluation_metrics(y_true, y_pred, model_name):\n",
        "    print(\"Evaluation metrics for\", model_name)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Confusion Matrix for\", model_name)\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Logistic Regression\n",
        "print_evaluation_metrics(test_labels, logistic_regression_pred, \"Logistic Regression\")\n",
        "\n",
        "# SVM\n",
        "print_evaluation_metrics(test_labels, svm_pred, \"SVM\")\n",
        "\n",
        "# Random Forest\n",
        "print_evaluation_metrics(test_labels, random_forest_pred, \"Random Forest\")\n",
        "\n",
        "#knn\n",
        "print_evaluation_metrics(test_labels, knn_pred, \"K-Nearest Neighbors\")\n",
        "\n",
        "#decision\n",
        "print_evaluation_metrics(test_labels, decision_tree_pred, \"Decision Trees\")\n",
        "\n",
        "#gradient boosting\n",
        "print_evaluation_metrics(test_labels, xgb_pred, \"XGBoost\")\n",
        "\n",
        "#naive bayes\n",
        "print_evaluation_metrics(test_labels, naive_bayes_pred, \"Naive Bayes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4h51vSvO00Y",
        "outputId": "82559c21-b5e8-403a-eaf2-fbecaf09877d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation metrics for Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93      2293\n",
            "           1       0.94      0.95      0.95      2705\n",
            "           2       0.87      0.85      0.86      2436\n",
            "           3       0.87      0.85      0.86      2533\n",
            "           4       0.88      0.89      0.88      2328\n",
            "           5       0.84      0.83      0.83      2131\n",
            "           6       0.91      0.91      0.91      2348\n",
            "           7       0.90      0.90      0.90      2485\n",
            "           8       0.83      0.83      0.83      2344\n",
            "           9       0.84      0.86      0.85      2397\n",
            "\n",
            "    accuracy                           0.88     24000\n",
            "   macro avg       0.88      0.88      0.88     24000\n",
            "weighted avg       0.88      0.88      0.88     24000\n",
            "\n",
            "Confusion Matrix for Logistic Regression\n",
            "[[2141    1   30   10    6   41   21    8   21   14]\n",
            " [   0 2580   19   16    4   10    7   13   45   11]\n",
            " [  30   41 2068   69   32   20   40   43   77   16]\n",
            " [  23   15   86 2152   12   88   13   23   71   50]\n",
            " [  15   18   20   10 2061    7   43   31   25   98]\n",
            " [  32    9   19   85   43 1769   54    8   83   29]\n",
            " [  22    7   55    4   39   51 2140    2   25    3]\n",
            " [  14   10   42   29   34    8    3 2225   16  104]\n",
            " [  19   55   42   83   13   85   18   13 1954   62]\n",
            " [  12   16    7   25   93   33    2  108   32 2069]]\n",
            "\n",
            "\n",
            "Evaluation metrics for SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      2293\n",
            "           1       0.98      0.99      0.98      2705\n",
            "           2       0.96      0.97      0.97      2436\n",
            "           3       0.97      0.96      0.96      2533\n",
            "           4       0.96      0.98      0.97      2328\n",
            "           5       0.96      0.96      0.96      2131\n",
            "           6       0.98      0.98      0.98      2348\n",
            "           7       0.98      0.97      0.97      2485\n",
            "           8       0.96      0.96      0.96      2344\n",
            "           9       0.97      0.96      0.96      2397\n",
            "\n",
            "    accuracy                           0.97     24000\n",
            "   macro avg       0.97      0.97      0.97     24000\n",
            "weighted avg       0.97      0.97      0.97     24000\n",
            "\n",
            "Confusion Matrix for SVM\n",
            "[[2267    2    3    0    0    4    5    0   10    2]\n",
            " [   0 2666   10    8    4    1    1    9    4    2]\n",
            " [   6    4 2366    5   12    2    7   15   18    1]\n",
            " [   2    3   31 2428    0   27    2    6   28    6]\n",
            " [   0    4   10    0 2274    1    7    1    2   29]\n",
            " [   3    3    7   29    8 2052   16    0    8    5]\n",
            " [   6    5    1    0    3   19 2309    0    5    0]\n",
            " [   0   10   19    1   13    4    0 2414    3   21]\n",
            " [   3   11    8   20    4   23    8    5 2248   14]\n",
            " [   7    4    1   14   41    8    0   21    8 2293]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98      2293\n",
            "           1       0.98      0.98      0.98      2705\n",
            "           2       0.95      0.95      0.95      2436\n",
            "           3       0.95      0.94      0.95      2533\n",
            "           4       0.96      0.97      0.96      2328\n",
            "           5       0.96      0.95      0.95      2131\n",
            "           6       0.97      0.98      0.97      2348\n",
            "           7       0.97      0.96      0.97      2485\n",
            "           8       0.95      0.94      0.94      2344\n",
            "           9       0.94      0.94      0.94      2397\n",
            "\n",
            "    accuracy                           0.96     24000\n",
            "   macro avg       0.96      0.96      0.96     24000\n",
            "weighted avg       0.96      0.96      0.96     24000\n",
            "\n",
            "Confusion Matrix for Random Forest\n",
            "[[2257    1    1    2    1    2    7    1   20    1]\n",
            " [   0 2661   17    7    2    0    2    7    4    5]\n",
            " [  13    3 2324   18   18    4   10   17   24    5]\n",
            " [   2    8   38 2387    1   28    9   19   28   13]\n",
            " [   2    5    7    0 2249    0   13    2    8   42]\n",
            " [  10    2    7   32    6 2029   20    2   12   11]\n",
            " [   8    7    3    0    4   27 2292    0    6    1]\n",
            " [   1   10   26    1   14    2    0 2392    4   35]\n",
            " [   5   15   19   25    8   20   13    2 2194   43]\n",
            " [  11    5    5   32   36    9    1   26   17 2255]]\n",
            "\n",
            "\n",
            "Evaluation metrics for K-Nearest Neighbors\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98      2293\n",
            "           1       0.95      0.99      0.97      2705\n",
            "           2       0.99      0.95      0.97      2436\n",
            "           3       0.96      0.96      0.96      2533\n",
            "           4       0.98      0.96      0.97      2328\n",
            "           5       0.95      0.95      0.95      2131\n",
            "           6       0.98      0.99      0.98      2348\n",
            "           7       0.96      0.97      0.96      2485\n",
            "           8       0.98      0.91      0.94      2344\n",
            "           9       0.94      0.96      0.95      2397\n",
            "\n",
            "    accuracy                           0.96     24000\n",
            "   macro avg       0.96      0.96      0.96     24000\n",
            "weighted avg       0.96      0.96      0.96     24000\n",
            "\n",
            "Confusion Matrix for K-Nearest Neighbors\n",
            "[[2274    2    1    0    0    6    7    1    1    1]\n",
            " [   0 2685    5    2    2    0    3    4    0    4]\n",
            " [  22   35 2311    7    2    2    2   36   15    4]\n",
            " [   1   14   14 2432    2   29    2   17   14    8]\n",
            " [   3   22    1    0 2232    0    7    5    0   58]\n",
            " [   9    5    0   37    6 2035   20    3    6   10]\n",
            " [   8    7    1    0    2   15 2314    0    1    0]\n",
            " [   2   25    3    1    9    1    0 2414    1   29]\n",
            " [   8   36    9   47    8   50   15    8 2127   36]\n",
            " [   8    7    1   12   23    8    1   36    4 2297]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Decision Trees\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91      2293\n",
            "           1       0.93      0.94      0.93      2705\n",
            "           2       0.83      0.80      0.81      2436\n",
            "           3       0.80      0.82      0.81      2533\n",
            "           4       0.83      0.84      0.83      2328\n",
            "           5       0.79      0.78      0.79      2131\n",
            "           6       0.88      0.87      0.88      2348\n",
            "           7       0.87      0.89      0.88      2485\n",
            "           8       0.80      0.76      0.78      2344\n",
            "           9       0.78      0.82      0.80      2397\n",
            "\n",
            "    accuracy                           0.84     24000\n",
            "   macro avg       0.84      0.84      0.84     24000\n",
            "weighted avg       0.84      0.84      0.84     24000\n",
            "\n",
            "Confusion Matrix for Decision Trees\n",
            "[[2066    2   31   19   12   54   50   11   32   16]\n",
            " [   3 2532   41   20   11   20    5   28   30   15]\n",
            " [  27   33 1954  112   47   19   51   70   83   40]\n",
            " [  19   32   85 2075   18  112   13   30   82   67]\n",
            " [   8   15   32   28 1946   31   33   40   43  152]\n",
            " [  41   20   38  117   33 1669   54   25   59   75]\n",
            " [  28   11   45   20   45   66 2049    8   59   17]\n",
            " [   2   17   38   24   47   13    7 2212   30   95]\n",
            " [  31   57   71  117   53   76   47   31 1793   68]\n",
            " [  20   11   27   62  130   57    8   74   44 1964]]\n",
            "\n",
            "\n",
            "Evaluation metrics for XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98      2293\n",
            "           1       0.98      0.98      0.98      2705\n",
            "           2       0.96      0.97      0.96      2436\n",
            "           3       0.97      0.96      0.96      2533\n",
            "           4       0.97      0.97      0.97      2328\n",
            "           5       0.96      0.97      0.97      2131\n",
            "           6       0.98      0.98      0.98      2348\n",
            "           7       0.97      0.97      0.97      2485\n",
            "           8       0.96      0.96      0.96      2344\n",
            "           9       0.96      0.96      0.96      2397\n",
            "\n",
            "    accuracy                           0.97     24000\n",
            "   macro avg       0.97      0.97      0.97     24000\n",
            "weighted avg       0.97      0.97      0.97     24000\n",
            "\n",
            "Confusion Matrix for XGBoost\n",
            "[[2254    1    4    1    3    6    3    0   18    3]\n",
            " [   0 2660   15    7    5    1    1   10    3    3]\n",
            " [   8    3 2352   15   12    2    8   17   15    4]\n",
            " [   1    6   31 2424    4   22    2   11   21   11]\n",
            " [   4    5    9    1 2263    0    8    3    4   31]\n",
            " [   5    1    6   20    6 2063    8    5   12    5]\n",
            " [  10    5    0    2    7   25 2291    0    8    0]\n",
            " [   4    7   24    1   10    2    0 2412    5   20]\n",
            " [   7    6   10   11    3   13   13    1 2251   29]\n",
            " [   5    7    3   11   27    7    1   24   12 2300]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.92      0.79      2293\n",
            "           1       0.73      0.94      0.82      2705\n",
            "           2       0.89      0.29      0.44      2436\n",
            "           3       0.73      0.31      0.43      2533\n",
            "           4       0.79      0.19      0.30      2328\n",
            "           5       0.58      0.05      0.08      2131\n",
            "           6       0.71      0.91      0.80      2348\n",
            "           7       0.90      0.36      0.51      2485\n",
            "           8       0.28      0.70      0.40      2344\n",
            "           9       0.43      0.91      0.59      2397\n",
            "\n",
            "    accuracy                           0.56     24000\n",
            "   macro avg       0.67      0.56      0.52     24000\n",
            "weighted avg       0.68      0.56      0.52     24000\n",
            "\n",
            "Confusion Matrix for Naive Bayes\n",
            "[[2102    6    9    5    4    6   33    0  117   11]\n",
            " [   1 2553    7   14    2    3   27    0   75   23]\n",
            " [ 276   75  710  189   17   13  507    9  616   24]\n",
            " [ 202  169   21  773    9    3   82   23 1107  144]\n",
            " [  66   51   16    9  433   14  117   11  652  959]\n",
            " [ 304   87   11   38   16   97   68    5 1368  137]\n",
            " [  40   64   11    1    0   12 2145    0   75    0]\n",
            " [  13   31    2   14   28    2    6  883  123 1383]\n",
            " [  42  414    6   17   13   14   26    6 1632  174]\n",
            " [  11   63    4    5   24    4    0   44   65 2177]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For, 50:50 split, As can be seen from above results,  SVM and XGBoost demonstrate the highest accuracies at 97%, showcasing their effectiveness in classification tasks. Conversely, Naive Bayes lags with 56% accuracy, suggesting limitations in handling complex data patterns."
      ],
      "metadata": {
        "id": "P4knI2PgHWLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#20:80 split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(train_images, train_labels, test_size=0.8, random_state=42)\n",
        "\n",
        "print(\"Training images:\", train_images.shape)\n",
        "print(\"Training labels:\", train_labels.shape)\n",
        "print(\"Testing images:\", test_images.shape)\n",
        "print(\"Testing labels:\", test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDOx1vAwO_Og",
        "outputId": "9fa21ced-b38e-4ad0-a62a-62374f841452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: (4800, 28, 28)\n",
            "Training labels: (4800,)\n",
            "Testing images: (19200, 28, 28)\n",
            "Testing labels: (19200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#gradient boosting\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize models\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "svm_classifier = SVC()\n",
        "random_forest = RandomForestClassifier()\n",
        "knn = KNeighborsClassifier()\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "xgb = XGBClassifier()\n",
        "naive_bayes = GaussianNB()\n",
        "\n",
        "# Fit the models\n",
        "\n",
        "\n",
        "logistic_regression.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "svm_classifier.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "random_forest.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "knn.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "decision_tree.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "xgb.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "naive_bayes.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "\n",
        "# Predictions\n",
        "logistic_regression_pred = logistic_regression.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "svm_pred = svm_classifier.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "random_forest_pred = random_forest.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "knn_pred = knn.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "decision_tree_pred = decision_tree.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "xgb_pred = xgb.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "naive_bayes_pred = naive_bayes.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "\n",
        "# Accuracy\n",
        "logistic_regression_accuracy = accuracy_score(test_labels, logistic_regression_pred)\n",
        "svm_accuracy = accuracy_score(test_labels, svm_pred)\n",
        "random_forest_accuracy = accuracy_score(test_labels, random_forest_pred)\n",
        "knn_accuracy = accuracy_score(test_labels, knn_pred)\n",
        "decision_tree_accuracy = accuracy_score(test_labels, decision_tree_pred)\n",
        "xgb_accuracy = accuracy_score(test_labels, xgb_pred)\n",
        "naive_bayes_accuracy = accuracy_score(test_labels, naive_bayes_pred)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", logistic_regression_accuracy)\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"Random Forest Accuracy:\", random_forest_accuracy)\n",
        "print(\"knn Accuracy:\", knn_accuracy)\n",
        "print(\"decision_tree Accuracy:\", decision_tree_accuracy)\n",
        "print(\"xgb Accuracy:\", xgb_accuracy)\n",
        "print(\"naive_bayes Accuracy:\", naive_bayes_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5f9xmaQPNoX",
        "outputId": "93de10dc-72b9-4d07-afcb-ea568482b998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.86546875\n",
            "SVM Accuracy: 0.9472916666666666\n",
            "Random Forest Accuracy: 0.9363541666666667\n",
            "knn Accuracy: 0.9320833333333334\n",
            "decision_tree Accuracy: 0.7680729166666667\n",
            "xgb Accuracy: 0.9360416666666667\n",
            "naive_bayes Accuracy: 0.5686979166666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For, **20:80 split**, As can be seen from above results, SVM has shown best accuracy as compared to other models."
      ],
      "metadata": {
        "id": "fI5ipYAAQGo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Function to print metrics and confusion matrix\n",
        "def print_evaluation_metrics(y_true, y_pred, model_name):\n",
        "    print(\"Evaluation metrics for\", model_name)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Confusion Matrix for\", model_name)\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Logistic Regression\n",
        "print_evaluation_metrics(test_labels, logistic_regression_pred, \"Logistic Regression\")\n",
        "\n",
        "# SVM\n",
        "print_evaluation_metrics(test_labels, svm_pred, \"SVM\")\n",
        "\n",
        "# Random Forest\n",
        "print_evaluation_metrics(test_labels, random_forest_pred, \"Random Forest\")\n",
        "\n",
        "#knn\n",
        "print_evaluation_metrics(test_labels, knn_pred, \"K-Nearest Neighbors\")\n",
        "\n",
        "#decision\n",
        "print_evaluation_metrics(test_labels, decision_tree_pred, \"Decision Trees\")\n",
        "\n",
        "#gradient boosting\n",
        "print_evaluation_metrics(test_labels, xgb_pred, \"XGBoost\")\n",
        "\n",
        "#naive bayes\n",
        "print_evaluation_metrics(test_labels, naive_bayes_pred, \"Naive Bayes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4_p4SgkQNZY",
        "outputId": "0dee48f1-2ccd-4ba7-e505-d1935975323d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation metrics for Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.93      0.94      1985\n",
            "           1       0.94      0.96      0.95      2179\n",
            "           2       0.83      0.86      0.84      1865\n",
            "           3       0.83      0.82      0.83      1894\n",
            "           4       0.87      0.87      0.87      1859\n",
            "           5       0.83      0.78      0.80      1760\n",
            "           6       0.89      0.91      0.90      1899\n",
            "           7       0.88      0.90      0.89      1993\n",
            "           8       0.82      0.78      0.80      1889\n",
            "           9       0.82      0.82      0.82      1877\n",
            "\n",
            "    accuracy                           0.87     19200\n",
            "   macro avg       0.86      0.86      0.86     19200\n",
            "weighted avg       0.87      0.87      0.87     19200\n",
            "\n",
            "Confusion Matrix for Logistic Regression\n",
            "[[1855    0   45    6    4   26   27    3   14    5]\n",
            " [   1 2083   18   12    2   10    7    9   33    4]\n",
            " [  19   11 1606   55   28   14   46   31   45   10]\n",
            " [   8   12   72 1555    7   90   13   43   60   34]\n",
            " [  13   12   11   10 1610    3   50   17   24  109]\n",
            " [  34   20   31   86   42 1377   44   13   85   28]\n",
            " [  12    9   53   11   23   39 1719    6   13   14]\n",
            " [   5    9   20   30   22    4    3 1794   13   93]\n",
            " [  22   54   72   78   22   79   21   18 1470   53]\n",
            " [   8   13   18   31   93   25    5  108   28 1548]]\n",
            "\n",
            "\n",
            "Evaluation metrics for SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98      1985\n",
            "           1       0.96      0.98      0.97      2179\n",
            "           2       0.93      0.94      0.94      1865\n",
            "           3       0.95      0.92      0.93      1894\n",
            "           4       0.94      0.95      0.95      1859\n",
            "           5       0.93      0.93      0.93      1760\n",
            "           6       0.96      0.98      0.97      1899\n",
            "           7       0.96      0.95      0.95      1993\n",
            "           8       0.95      0.91      0.93      1889\n",
            "           9       0.93      0.91      0.92      1877\n",
            "\n",
            "    accuracy                           0.95     19200\n",
            "   macro avg       0.95      0.95      0.95     19200\n",
            "weighted avg       0.95      0.95      0.95     19200\n",
            "\n",
            "Confusion Matrix for SVM\n",
            "[[1961    1    6    0    1    2   10    0    4    0]\n",
            " [   1 2140   13    4    3    4    2    2    9    1]\n",
            " [  17    6 1762   10   24    6   12   11   13    4]\n",
            " [   1    7   36 1739    1   46    4   20   30   10]\n",
            " [   6    7   11    1 1772    0   10    5    3   44]\n",
            " [  13   11    7   38    7 1640   28    2   11    3]\n",
            " [   9    5   10    0   11    7 1855    0    2    0]\n",
            " [   2    9   14    2   15    2    0 1890    5   54]\n",
            " [  13   38   18   18   16   38   12    7 1713   16]\n",
            " [   3    8   13   25   41   10    3   39   19 1716]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      1985\n",
            "           1       0.96      0.98      0.97      2179\n",
            "           2       0.92      0.93      0.93      1865\n",
            "           3       0.93      0.91      0.92      1894\n",
            "           4       0.93      0.93      0.93      1859\n",
            "           5       0.94      0.92      0.93      1760\n",
            "           6       0.94      0.98      0.96      1899\n",
            "           7       0.95      0.93      0.94      1993\n",
            "           8       0.94      0.89      0.92      1889\n",
            "           9       0.88      0.91      0.89      1877\n",
            "\n",
            "    accuracy                           0.94     19200\n",
            "   macro avg       0.94      0.94      0.94     19200\n",
            "weighted avg       0.94      0.94      0.94     19200\n",
            "\n",
            "Confusion Matrix for Random Forest\n",
            "[[1951    1    5    1    0    2   15    0    9    1]\n",
            " [   0 2134   14    5    5    6   10    1    4    0]\n",
            " [  18    3 1742   11   26    4   21   20   15    5]\n",
            " [   5    3   42 1715    5   39    6   27   34   18]\n",
            " [   7    5    7    3 1726    2   20    5    8   76]\n",
            " [  18   11    3   47   10 1614   23    3   12   19]\n",
            " [   8    9    9    1    6   10 1852    1    3    0]\n",
            " [   3    9   32    4   18    1    0 1851    5   70]\n",
            " [  10   33   29   20   13   36   16    4 1690   38]\n",
            " [   9    7   16   36   45    6    3   35   17 1703]]\n",
            "\n",
            "\n",
            "Evaluation metrics for K-Nearest Neighbors\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97      1985\n",
            "           1       0.89      0.99      0.94      2179\n",
            "           2       0.97      0.90      0.94      1865\n",
            "           3       0.92      0.93      0.92      1894\n",
            "           4       0.94      0.90      0.92      1859\n",
            "           5       0.92      0.91      0.92      1760\n",
            "           6       0.95      0.98      0.97      1899\n",
            "           7       0.92      0.95      0.93      1993\n",
            "           8       0.98      0.84      0.91      1889\n",
            "           9       0.88      0.91      0.89      1877\n",
            "\n",
            "    accuracy                           0.93     19200\n",
            "   macro avg       0.93      0.93      0.93     19200\n",
            "weighted avg       0.93      0.93      0.93     19200\n",
            "\n",
            "Confusion Matrix for K-Nearest Neighbors\n",
            "[[1967    0    0    1    0    4    9    0    2    2]\n",
            " [   1 2166    5    1    3    0    1    0    2    0]\n",
            " [  31   45 1687   11   10    6   14   51    7    3]\n",
            " [   6   15   19 1753    2   43    2   26   14   14]\n",
            " [   5   43    0    0 1677    1   13    9    0  111]\n",
            " [  18   17    3   51    7 1608   29    1    5   21]\n",
            " [  17   14    1    0    2    7 1856    0    2    0]\n",
            " [   2   44    5    1   13    0    0 1889    0   39]\n",
            " [  15   69   11   52   14   67   21   11 1593   36]\n",
            " [   9   16    4   28   47    5    2   61    5 1700]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Decision Trees\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84      1985\n",
            "           1       0.88      0.92      0.90      2179\n",
            "           2       0.71      0.74      0.72      1865\n",
            "           3       0.73      0.71      0.72      1894\n",
            "           4       0.75      0.78      0.76      1859\n",
            "           5       0.67      0.65      0.66      1760\n",
            "           6       0.79      0.79      0.79      1899\n",
            "           7       0.82      0.84      0.83      1993\n",
            "           8       0.74      0.67      0.70      1889\n",
            "           9       0.71      0.72      0.71      1877\n",
            "\n",
            "    accuracy                           0.77     19200\n",
            "   macro avg       0.76      0.76      0.76     19200\n",
            "weighted avg       0.77      0.77      0.77     19200\n",
            "\n",
            "Confusion Matrix for Decision Trees\n",
            "[[1653    5   71   34   24   63   64   22   23   26]\n",
            " [   0 1998   56   40    4   10    8   29   24   10]\n",
            " [  32   52 1377   67   50   57   77   48   73   32]\n",
            " [  44   42   67 1341   34  153   23   32   79   79]\n",
            " [  20   17   43   22 1441   32   53   43   67  121]\n",
            " [  61   34   44  154   44 1150   85   37   85   66]\n",
            " [  56   13  112   18   76   55 1491   13   39   26]\n",
            " [   6   21   36   54   45   21   11 1678   17  104]\n",
            " [  50   72   93   74   66   90   54   36 1269   85]\n",
            " [  20   14   37   36  136   86   29  120   50 1349]]\n",
            "\n",
            "\n",
            "Evaluation metrics for XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      1985\n",
            "           1       0.96      0.97      0.97      2179\n",
            "           2       0.92      0.92      0.92      1865\n",
            "           3       0.94      0.90      0.92      1894\n",
            "           4       0.93      0.92      0.92      1859\n",
            "           5       0.94      0.93      0.94      1760\n",
            "           6       0.94      0.97      0.95      1899\n",
            "           7       0.95      0.93      0.94      1993\n",
            "           8       0.93      0.91      0.92      1889\n",
            "           9       0.89      0.91      0.90      1877\n",
            "\n",
            "    accuracy                           0.94     19200\n",
            "   macro avg       0.94      0.94      0.94     19200\n",
            "weighted avg       0.94      0.94      0.94     19200\n",
            "\n",
            "Confusion Matrix for XGBoost\n",
            "[[1950    2    2    4    3    2    9    0   12    1]\n",
            " [   1 2118   22    5    5    5    8    5   10    0]\n",
            " [  14    8 1723   21   21    5   25   18   20   10]\n",
            " [   3    9   37 1711    6   39    6   21   38   24]\n",
            " [   5    5    8    3 1718    2   19    5   10   84]\n",
            " [  16    7    6   30    7 1633   27    6   16   12]\n",
            " [  10    5    9    0    7   18 1840    0    9    1]\n",
            " [   7   12   37    4   16    1    0 1860    7   49]\n",
            " [  10   34   21   17   15   19   24    7 1710   32]\n",
            " [   9    6   12   20   59    9    5   35   13 1709]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.78      0.77      1985\n",
            "           1       0.72      0.93      0.81      2179\n",
            "           2       0.82      0.39      0.53      1865\n",
            "           3       0.77      0.29      0.42      1894\n",
            "           4       0.75      0.20      0.32      1859\n",
            "           5       0.63      0.10      0.18      1760\n",
            "           6       0.70      0.91      0.79      1899\n",
            "           7       0.89      0.42      0.57      1993\n",
            "           8       0.27      0.68      0.39      1889\n",
            "           9       0.44      0.88      0.58      1877\n",
            "\n",
            "    accuracy                           0.57     19200\n",
            "   macro avg       0.67      0.56      0.54     19200\n",
            "weighted avg       0.68      0.57      0.54     19200\n",
            "\n",
            "Confusion Matrix for Naive Bayes\n",
            "[[1544    6   21    6    5   10   72    2  308   11]\n",
            " [   0 2032   11   13    4    8   21    2   75   13]\n",
            " [ 138   66  736   79    9   14  390    9  407   17]\n",
            " [  90  111   29  546   22    6   74   26  873  117]\n",
            " [  41   64   24    7  376   15   88    9  439  796]\n",
            " [ 152   68   20   26   20  179   73    5 1118   99]\n",
            " [  18   49   31    0    4    9 1729    0   56    3]\n",
            " [   7   21    3   17   28    3    2  837  100  975]\n",
            " [  39  371   20   14   11   28   16    3 1282  105]\n",
            " [  13   46    3    3   21   10    1   48   74 1658]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For, **20:80 split**, As can be seen from above results, Logistic Regression demonstrates high precision and recall for most classes, particularly achieving 0.94 precision for class 0 and 0.96 precision for class 1. However, it shows lower performance for classes 5 and 8, with precision scores of 0.83 and 0.82 respectively. The overall accuracy of 0.87 indicates its effectiveness in classification tasks."
      ],
      "metadata": {
        "id": "jtr1MpB0H5q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1:99 split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(train_images, train_labels, test_size=0.99, random_state=42)\n",
        "\n",
        "print(\"Training images:\", train_images.shape)\n",
        "print(\"Training labels:\", train_labels.shape)\n",
        "print(\"Testing images:\", test_images.shape)\n",
        "print(\"Testing labels:\", test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RC25HStQZ0o",
        "outputId": "c76e7095-a468-4af0-8a16-6c4c539b77d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: (48, 28, 28)\n",
            "Training labels: (48,)\n",
            "Testing images: (4752, 28, 28)\n",
            "Testing labels: (4752,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#gradient boosting\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize models\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "svm_classifier = SVC()\n",
        "random_forest = RandomForestClassifier()\n",
        "knn = KNeighborsClassifier()\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "xgb = XGBClassifier()\n",
        "naive_bayes = GaussianNB()\n",
        "\n",
        "# Fit the models\n",
        "\n",
        "\n",
        "logistic_regression.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "svm_classifier.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "random_forest.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "knn.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "decision_tree.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "xgb.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "naive_bayes.fit(train_images.reshape((train_images.shape[0], -1)), train_labels)\n",
        "\n",
        "# Predictions\n",
        "logistic_regression_pred = logistic_regression.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "svm_pred = svm_classifier.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "random_forest_pred = random_forest.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "knn_pred = knn.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "decision_tree_pred = decision_tree.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "xgb_pred = xgb.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "naive_bayes_pred = naive_bayes.predict(test_images.reshape((test_images.shape[0], -1)))\n",
        "\n",
        "# Accuracy\n",
        "logistic_regression_accuracy = accuracy_score(test_labels, logistic_regression_pred)\n",
        "svm_accuracy = accuracy_score(test_labels, svm_pred)\n",
        "random_forest_accuracy = accuracy_score(test_labels, random_forest_pred)\n",
        "knn_accuracy = accuracy_score(test_labels, knn_pred)\n",
        "decision_tree_accuracy = accuracy_score(test_labels, decision_tree_pred)\n",
        "xgb_accuracy = accuracy_score(test_labels, xgb_pred)\n",
        "naive_bayes_accuracy = accuracy_score(test_labels, naive_bayes_pred)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", logistic_regression_accuracy)\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"Random Forest Accuracy:\", random_forest_accuracy)\n",
        "print(\"knn Accuracy:\", knn_accuracy)\n",
        "print(\"decision_tree Accuracy:\", decision_tree_accuracy)\n",
        "print(\"xgb Accuracy:\", xgb_accuracy)\n",
        "print(\"naive_bayes Accuracy:\", naive_bayes_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcmDskqIQmSu",
        "outputId": "b18d175b-2791-47ce-d577-15e80be7003f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.6298400673400674\n",
            "SVM Accuracy: 0.5425084175084175\n",
            "Random Forest Accuracy: 0.5723905723905723\n",
            "knn Accuracy: 0.5437710437710438\n",
            "decision_tree Accuracy: 0.3263888888888889\n",
            "xgb Accuracy: 0.4377104377104377\n",
            "naive_bayes Accuracy: 0.5258838383838383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For, **1:99 split**, As can be seen from above results, Logistic Regression model has shown best accuracy as compared to other models."
      ],
      "metadata": {
        "id": "dK8ucMMJQwI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Function to print metrics and confusion matrix\n",
        "def print_evaluation_metrics(y_true, y_pred, model_name):\n",
        "    print(\"Evaluation metrics for\", model_name)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Confusion Matrix for\", model_name)\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Logistic Regression\n",
        "print_evaluation_metrics(test_labels, logistic_regression_pred, \"Logistic Regression\")\n",
        "\n",
        "# SVM\n",
        "print_evaluation_metrics(test_labels, svm_pred, \"SVM\")\n",
        "\n",
        "# Random Forest\n",
        "print_evaluation_metrics(test_labels, random_forest_pred, \"Random Forest\")\n",
        "\n",
        "#knn\n",
        "print_evaluation_metrics(test_labels, knn_pred, \"K-Nearest Neighbors\")\n",
        "\n",
        "#decision\n",
        "print_evaluation_metrics(test_labels, decision_tree_pred, \"Decision Trees\")\n",
        "\n",
        "#gradient boosting\n",
        "print_evaluation_metrics(test_labels, xgb_pred, \"XGBoost\")\n",
        "\n",
        "#naive bayes\n",
        "print_evaluation_metrics(test_labels, naive_bayes_pred, \"Naive Bayes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye3w-0nKRAVE",
        "outputId": "00b38c69-9019-465d-d786-be73db96d95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation metrics for Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.89      0.83       462\n",
            "           1       0.85      0.73      0.78       532\n",
            "           2       0.89      0.48      0.63       479\n",
            "           3       0.66      0.73      0.69       479\n",
            "           4       0.47      0.90      0.62       473\n",
            "           5       0.95      0.09      0.17       425\n",
            "           6       0.82      0.86      0.84       489\n",
            "           7       0.51      0.82      0.63       481\n",
            "           8       0.48      0.64      0.55       453\n",
            "           9       0.31      0.09      0.13       479\n",
            "\n",
            "    accuracy                           0.63      4752\n",
            "   macro avg       0.67      0.62      0.59      4752\n",
            "weighted avg       0.67      0.63      0.59      4752\n",
            "\n",
            "Confusion Matrix for Logistic Regression\n",
            "[[412   0   1  17   1   0  11   6  14   0]\n",
            " [  0 389   0   0   3   0   0   2  95  43]\n",
            " [ 12  33 232  14  41   0  26  86  34   1]\n",
            " [ 16  10   4 349  32   1  16  21  28   2]\n",
            " [  0   2   0   1 428   0  10  19   7   6]\n",
            " [ 54   4   0  99  66  39  26  53  79   5]\n",
            " [  6   3  21   4   9   1 419   0  26   0]\n",
            " [ 17   5   0   0  28   0   0 396   2  33]\n",
            " [  6  13   4  27  67   0   6  41 288   1]\n",
            " [  9   1   0  16 242   0   0 148  22  41]]\n",
            "\n",
            "\n",
            "Evaluation metrics for SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.91      0.72       462\n",
            "           1       0.89      0.63      0.74       532\n",
            "           2       0.97      0.30      0.45       479\n",
            "           3       0.51      0.82      0.63       479\n",
            "           4       0.45      0.65      0.53       473\n",
            "           5       0.00      0.00      0.00       425\n",
            "           6       0.91      0.65      0.76       489\n",
            "           7       0.32      0.94      0.47       481\n",
            "           8       0.71      0.47      0.56       453\n",
            "           9       0.00      0.00      0.00       479\n",
            "\n",
            "    accuracy                           0.54      4752\n",
            "   macro avg       0.53      0.54      0.49      4752\n",
            "weighted avg       0.54      0.54      0.49      4752\n",
            "\n",
            "Confusion Matrix for SVM\n",
            "[[419   0   0  23   1   0   7  11   1   0]\n",
            " [  0 336   0  10  32   0   0 139  15   0]\n",
            " [ 73  24 142  64  48   0  12  92  24   0]\n",
            " [ 20   2   3 391  11   0   4  40   8   0]\n",
            " [ 16   2   0   1 309   0   6 138   1   0]\n",
            " [ 76   1   0 149  30   0   1 134  34   0]\n",
            " [ 36   5   2  23  64   0 320  34   5   0]\n",
            " [ 12   2   0   0  17   0   0 450   0   0]\n",
            " [ 27   4   0  84  82   0   3  42 211   0]\n",
            " [ 23   0   0  17  96   0   0 343   0   0]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.87      0.76       462\n",
            "           1       0.75      0.79      0.77       532\n",
            "           2       0.88      0.31      0.46       479\n",
            "           3       0.50      0.65      0.57       479\n",
            "           4       0.41      0.84      0.55       473\n",
            "           5       0.00      0.00      0.00       425\n",
            "           6       0.71      0.75      0.73       489\n",
            "           7       0.49      0.80      0.61       481\n",
            "           8       0.56      0.59      0.57       453\n",
            "           9       0.40      0.04      0.08       479\n",
            "\n",
            "    accuracy                           0.57      4752\n",
            "   macro avg       0.54      0.56      0.51      4752\n",
            "weighted avg       0.55      0.57      0.52      4752\n",
            "\n",
            "Confusion Matrix for Random Forest\n",
            "[[404   0   1  16   3   0  20  14   4   0]\n",
            " [  0 421   0   2  14   0   0  39  50   6]\n",
            " [ 40  56 150  38  63   0  65  28  38   1]\n",
            " [ 17  18   3 311  28   0  14  43  43   2]\n",
            " [ 15   4   0   3 396   0  16  33   4   2]\n",
            " [ 34  11   1 150  61   0  11 112  44   1]\n",
            " [ 23  15  12  16  43   0 365   6   9   0]\n",
            " [ 16  10   0   3  44   0   1 385   2  20]\n",
            " [ 14  23   3  65  55   0  12  14 267   0]\n",
            " [ 35   5   0  12 270   0  10 111  15  21]]\n",
            "\n",
            "\n",
            "Evaluation metrics for K-Nearest Neighbors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.84      0.78       462\n",
            "           1       0.52      0.93      0.67       532\n",
            "           2       0.88      0.26      0.41       479\n",
            "           3       0.50      0.75      0.60       479\n",
            "           4       0.41      0.68      0.51       473\n",
            "           5       0.00      0.00      0.00       425\n",
            "           6       0.76      0.65      0.70       489\n",
            "           7       0.45      0.81      0.58       481\n",
            "           8       0.64      0.38      0.48       453\n",
            "           9       0.31      0.04      0.07       479\n",
            "\n",
            "    accuracy                           0.54      4752\n",
            "   macro avg       0.52      0.53      0.48      4752\n",
            "weighted avg       0.52      0.54      0.49      4752\n",
            "\n",
            "Confusion Matrix for K-Nearest Neighbors\n",
            "[[387   5   3  15   5   0  32   9   6   0]\n",
            " [  0 493   0   2   5   0   0   9   7  16]\n",
            " [ 21 159 126  37  45   0  37  33  20   1]\n",
            " [ 13  50   4 358  12   0   5  17  14   6]\n",
            " [  4  15   2   4 322   0  12 102   4   8]\n",
            " [ 66  42   0 168  47   0   7  59  30   6]\n",
            " [ 17  51   6  16  75   0 317   2   5   0]\n",
            " [  6  31   1   0  50   0   0 389   0   4]\n",
            " [ 11  93   1  95  58   0   7  12 172   4]\n",
            " [ 10   7   0  21 172   0   1 237  11  20]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Decision Trees\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.42      0.40       462\n",
            "           1       0.28      0.48      0.36       532\n",
            "           2       0.23      0.39      0.29       479\n",
            "           3       0.41      0.22      0.29       479\n",
            "           4       0.29      0.38      0.33       473\n",
            "           5       0.20      0.06      0.09       425\n",
            "           6       0.51      0.39      0.44       489\n",
            "           7       0.45      0.43      0.44       481\n",
            "           8       0.41      0.32      0.36       453\n",
            "           9       0.19      0.13      0.15       479\n",
            "\n",
            "    accuracy                           0.33      4752\n",
            "   macro avg       0.33      0.32      0.31      4752\n",
            "weighted avg       0.34      0.33      0.32      4752\n",
            "\n",
            "Confusion Matrix for Decision Trees\n",
            "[[194  29  80  29   8  40  45  13  10  14]\n",
            " [  1 258   2  30  47   0   1  84  43  66]\n",
            " [ 43  60 185   6  29  33  80   4  31   8]\n",
            " [ 15  66 165 107  66   8   5  12  17  18]\n",
            " [ 97  83  21   2 180   1  23  23  16  27]\n",
            " [ 38  57 116  53  34  25   8  44  16  34]\n",
            " [ 30  27 117   6  24  12 190  18  57   8]\n",
            " [ 23  95  11   9  61   3   6 209   3  61]\n",
            " [ 20 100  73  16  58   3  15   7 143  18]\n",
            " [ 39 141  46   5 124   0   2  46  16  60]]\n",
            "\n",
            "\n",
            "Evaluation metrics for XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.69      0.63       462\n",
            "           1       0.64      0.46      0.54       532\n",
            "           2       0.45      0.23      0.31       479\n",
            "           3       0.48      0.41      0.44       479\n",
            "           4       0.38      0.73      0.50       473\n",
            "           5       0.53      0.02      0.04       425\n",
            "           6       0.51      0.46      0.49       489\n",
            "           7       0.32      0.75      0.45       481\n",
            "           8       0.39      0.56      0.46       453\n",
            "           9       0.37      0.03      0.05       479\n",
            "\n",
            "    accuracy                           0.44      4752\n",
            "   macro avg       0.47      0.43      0.39      4752\n",
            "weighted avg       0.47      0.44      0.40      4752\n",
            "\n",
            "Confusion Matrix for XGBoost\n",
            "[[321   1  15  13  13   6  52  40   1   0]\n",
            " [  0 246   1  25  98   0   0  90  72   0]\n",
            " [ 62  25 111  21  75   0  80  72  33   0]\n",
            " [ 11  53  32 195  31   0  13  46  96   2]\n",
            " [  7  15   2   8 345   0   6  70  19   1]\n",
            " [ 10  18  41  79  79   9  29  92  65   3]\n",
            " [ 20   3  26   6 120   0 226  77  11   0]\n",
            " [ 29   8   0   2  50   0   2 361  12  17]\n",
            " [ 37   7   6  45  54   1  21  29 252   1]\n",
            " [ 56   8  13  10  42   1  10 234  91  14]]\n",
            "\n",
            "\n",
            "Evaluation metrics for Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.95      0.65       462\n",
            "           1       0.97      0.21      0.34       532\n",
            "           2       0.66      0.77      0.71       479\n",
            "           3       0.65      0.41      0.50       479\n",
            "           4       0.47      0.75      0.58       473\n",
            "           5       0.00      0.00      0.00       425\n",
            "           6       0.82      0.79      0.80       489\n",
            "           7       0.53      0.75      0.62       481\n",
            "           8       0.29      0.62      0.39       453\n",
            "           9       0.00      0.00      0.00       479\n",
            "\n",
            "    accuracy                           0.53      4752\n",
            "   macro avg       0.49      0.53      0.46      4752\n",
            "weighted avg       0.50      0.53      0.46      4752\n",
            "\n",
            "Confusion Matrix for Naive Bayes\n",
            "[[438   0   8   0   1   0   5   0  10   0]\n",
            " [  0 111  11   1   6   0   7  24 372   0]\n",
            " [ 41   3 368   0   5   0  26  14  22   0]\n",
            " [ 98   0  48 197  27   0  13  61  35   0]\n",
            " [ 26   0  24   2 357   0  11  20  33   0]\n",
            " [119   0  16  81  46   0  12   9 142   0]\n",
            " [ 45   0  41   2   5   0 385   0  11   0]\n",
            " [ 57   0   3   0  52   0   0 360   9   0]\n",
            " [ 31   0  29  13  56   0   9  32 283   0]\n",
            " [ 32   0   6   6 212   0   0 156  67   0]]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For, **1:99 split**, As can be seen from above results, Logistic Regression demonstrates good precision and recall for some classes but struggles with others, achieving an overall moderate accuracy of 63%. SVM shows varying precision and recall across classes, resulting in an accuracy of 54%. Random Forest achieves a slightly higher accuracy of 57% with balanced precision and recall. K-Nearest Neighbors and XGBoost perform similarly, with an accuracy of 54% and 44% respectively, showing varying performance across classes. Naive Bayes, despite its simplicity, achieves an accuracy of 53%, demonstrating its effectiveness in some classes but weakness in others."
      ],
      "metadata": {
        "id": "m26f3EhBItKk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}